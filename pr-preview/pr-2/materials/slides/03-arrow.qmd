---
title: ""
execute:
  echo: true
format: 
  revealjs:
    footer: "[JSM: Large Data](https://github.com/kbodwin/jsm-large-data)"
    theme: simple
    # scrollable: true
    embed-resources: true
editor: source
---

<style>
.extrapad {
    margin-top:1em  !important;
}
</style>

## Section Overview

1. **Introduction to Column-Oriented Data Storage**
   <!-- - The big data interchange problem
   - Row vs. column storage formats
   - Introduction to Parquet and Arrow -->

2. **Deep Dive into Parquet** 
   <!-- - Key features and benefits
   - Performance benchmarks
   - Demo: Efficient dataset storage -->

3. **Working with Arrow in R** 
   <!-- - The Arrow package
   - Reading/writing Parquet files
   - Using dplyr with Arrow tables -->

4. **Querying Parquet with Different Engines** 
   <!-- - DuckDB: SQL queries on Parquet files
   - Data.table approach
   - Arrow query execution
   - Performance comparisons -->

5. **Arrow Datasets for Larger-than-Memory Operations** 
   <!-- - Datasets vs. Tables
   - Handling data too large for memory
   - Working with datasets on S3 -->

6. **Partitioning Strategies** 
   <!-- - Understanding partitioning
   - Hive vs. non-Hive partitioning
   - Best practices -->

7. **Hands-on Workshop: Analysis with PUMS Data** 
   <!-- - Reading partitioned Parquet files
   - Executing queries with DuckDB and Arrow
   - Practice exercises -->

# Introduction to Column-Oriented Data Storage

## Why should I care about data storage?

::: {.fragment .extrapad}
Data has to be represented somewhere, both during analysis and when storing.
:::

::: {.fragment .extrapad}
The shape and characteristics of this representation has a huge impact on performance.
:::

::: {.fragment .extrapad}
What if you could speed up a key part of your analysis by 30x and reduce your storage by 10x?
:::

## Row vs. Column-Oriented Storage

::: columns
::: {.column width="50%"}
**Row-oriented**

```
|ID|Name |Age|City    |
|--|-----|---|--------|
|1 |Alice|25 |New York|
|2 |Bob  |30 |Boston  |
|3 |Carol|45 |Chicago |
```

- Efficient for single record access
- Efficient for appending
:::

::: {.column width="50%"}
::: {.fragment}
**Column-oriented**

```
ID:    [1, 2, 3]
Name:  [Alice, Bob, Carol]
Age:   [25, 30, 45]
City:  [New York, Boston, Chicago]
```

- Efficient for analytics
- Better compression
:::
:::
:::

::: {.notes}
Row oriented formats are super familiar: CSVs as well as many databases

But Column-orientation isn't something that is new and cutting edge. In fact, every single one of you use a system that stores data this way: R data frames(!)
:::

## Why Column-Oriented Storage?

::: {.incremental}
- **Analytics typically access a subset of columns**
  - "What is the average age by city?"
  - Only needs [Age, City] columns

- **Benefits:**
  - Only read needed columns from disk
  - Similar data types stored together
  - Better compression ratios
:::

::: {.notes}
Compression: this is because like-types are stored with like, so you get more frequent patterns — the core of compression. But you also can use encodings like dictionary encodings very efficiently.
:::

## Column-Oriented Data is great 

:::{.extrapad}
And you use column-oriented dataframes already!
:::

::: {.fragment}
... but still storing my data in a fundamentally row-oriented way. 
:::

::: {.notes}
This isn't so bad if you're only talking about a small amount of data, transposing a few columns for a few rows is no big deal. But as data gets larger, or if you have to do this frequently, this process of transposing (AKA serialization) hurts.
:::

## The interconnection problem

![](images/copy-convert.png){.r-stretch}

::: {.notes}
Many of these were operating in essentially column-oriented ways — but to transfer data ended up writting into row-oriented data structures, then read them back in to something that was column-oriented.

**Moving data between representations is hard**
  - Different formats, requirements, and limitations
  - Compatibility issues
  - Serialization is a huge bottleneck
:::

## The interconnection problem

![](images/shared.png){.r-stretch}

## What is Apache Arrow?

![](images/arrow-logo_vertical_black-txt_transparent-bg.png){width=50%}

::: {.incremental}
- **Cross-language development platform for in-memory data**
  - Consistent in-memory columnar data format
  - Language-independent
  - Zero-copy reads

- **Benefits:**
  - Seamless data interchange between systems
  - Fast analytical processing
  - Efficient memory usage
:::

## What is Apache Parquet?

![](images/Apache_Parquet_logo.svg){width=40%}

::: {.incremental}
- **Open-source columnar storage format**
  - Created by Twitter and Cloudera in 2013
  - Part of the Apache Software Foundation

- **Features:**
  - Columnar storage
  - Efficient compression
  - Explicit schema 
  - Statistical metadata
:::

## Reading a File

As a CSV file

```{r}
#| eval: false
system.time({
  df <- read.csv("CA_person_2021.csv")
})
```
:::{.fragment .extrapad}
```
   user  system elapsed 
 14.449   0.445  15.037 
```
:::

:::{.notes}
Describe the CSV

This CSV is 708 MB, I'm reading this in on my MacBook Pro, your times will vary! We can use arrow or data.tables's CSV reader and it's faster (1.85 sec and 1.61 sec respectively). And if we read to an arrow table it's even faster: 0.51 seconds
:::

## Reading a File

As a Parquet file

```{r}
#| eval: false
#| code-line-numbers: '|2|'
library(arrow)
options(arrow.use_altrep = FALSE)

system.time({
  df <- read_parquet("CA_person_2021.parquet")
})
```
:::{.fragment .extrapad}
```
   user  system elapsed 
  1.017   0.207   0.568 
```
:::

:::{.notes}
The parquet file is 62 MB

It's even faster with altrep (0.186 s), but that's cheating! Also, if we read into an arrow table rather than a dataframe: 0.1 second 
:::

# Deep Dive into Parquet

## What is Parquet?

::: {.incremental}
- **Schema metadata**
  - Self-describing format
  - Preserves column types
  - Type-safe data interchange

- **Encodings**
  - **Dictionary** — Particularly effective for categorical data
  - **Run-length encoding** - Efficient storage of sequential repeated values

- **Advanced compression**
  - Column-specific compression algorithms
  - Both dictionary and value compression
:::

## Exercise

```{r}
#| eval: false
data <- tibble::tibble(
  integers = 1:10,
  doubles = as.numeric(1:10),
  strings = sprintf("%02d", 1:10)
)

write.csv(data, "numeric_base.csv", row.names = FALSE)
write_csv_arrow(data, "numeric_arrow.csv")
write_parquet(data, "numeric.parquet")

df_csv <- read.csv("numeric_base.csv")
df_csv_arrow <- read_csv_arrow("numeric_arrow.csv")
df_parquet <- read_parquet("numeric.parquet")
```

::: {.fragment .extrapad}
Are there any differences?
:::

## Exercise

:::{.columns}
:::{.column}
```{.default}
> df_csv_arrow
# A tibble: 10 × 3
   integers doubles strings
      <int>   <int>   <int>
 1        1       1       1
 2        2       2       2
 3        3       3       3
 4        4       4       4
 5        5       5       5
 6        6       6       6
 7        7       7       7
 8        8       8       8
 9        9       9       9
10       10      10      10
```
:::

:::{.column}
```{.default}
> df_parquet
# A tibble: 10 × 3
   integers doubles strings
      <int>   <dbl> <chr>  
 1        1       1 01     
 2        2       2 02     
 3        3       3 03     
 4        4       4 04     
 5        5       5 05     
 6        6       6 06     
 7        7       7 07     
 8        8       8 08     
 9        9       9 09     
10       10      10 10     
```
:::
:::

## Exercise

:::{.columns}
:::{.column}
```{.default code-line-numbers="3-4"}
> df_csv_arrow
# A tibble: 10 × 3
   integers doubles strings
      <int>   <int>   <int>
 1        1       1       1
 2        2       2       2
 3        3       3       3
 4        4       4       4
 5        5       5       5
 6        6       6       6
 7        7       7       7
 8        8       8       8
 9        9       9       9
10       10      10      10
```
:::

:::{.column}
```{.default code-line-numbers="3-4"}
> df_parquet
# A tibble: 10 × 3
   integers doubles strings
      <int>   <dbl> <chr>  
 1        1       1 01     
 2        2       2 02     
 3        3       3 03     
 4        4       4 04     
 5        5       5 05     
 6        6       6 06     
 7        7       7 07     
 8        8       8 08     
 9        9       9 09     
10       10      10 10     
```
:::
:::

## Inside a Parquet File

![](images/files_formats_parquet_bw.png){width=100%}

::: {.notes}
- **Row groups**: Horizontal partitions of data
- **Column chunks**: Columnar data within a row group
- **Pages**: Small units of column chunk data
- **Footer**: Contains file metadata and schema
:::

## Benchmarks: Parquet vs CSV

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 4.25
#| fig-height: 5
library(ggplot2)
data <- data.frame(
  Format = c("CSV", "CSV (compressed)", "Parquet"),
  Size_MB = c(707.9, 101.6, 62)
)
ggplot(data, aes(x = Format, y = Size_MB, fill = Format)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(y = "Size (MB)", title = "File Size Comparison") + 
  guides(fill="none")
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 4.25
#| fig-height: 5
data <- data.frame(
  Format = c("CSV", "CSV (arrow)", "CSV (compressed)", "Parquet"),
  Time_Seconds = c(15.037, 1.806, 1.865,  0.568)
)
ggplot(data, aes(x = Format, y = Time_Seconds, color = Format)) +
  geom_point(stat = "identity", size = 3) +
  theme_minimal() +
  labs(y = "Time (seconds)", title = "Read Performance") + 
  guides(color="none")
```
:::
:::

## Benchmarks: Parquet vs CSV

```{r}
#| echo: false
#| fig-width: 7.5
#| fig-height: 3
data <- data.frame(
  Format = c("CSV (arrow)", "CSV (arrow, compressed)", "CSV (dt)", "CSV (dt, compressed)", "Parquet"),
  Time_Seconds = c(1.806, 1.865, 1.851, 2.325, 0.568)
)
ggplot(data, aes(x = Format, y = Time_Seconds, color = Format)) +
  geom_point(stat = "identity", size = 3) +
  theme_minimal() +
  labs(y = "Time (seconds)", title = "Read Performance, modern packages") + 
  guides(color="none")
```

## Reading Efficiency: Selecting Columns

::: {.incremental}
- **With CSV:**
  - Must read entire file, even if you only need a few columns
  - No efficient way to skip columns during read

- **With Parquet:**
  - Read only needed columns from disk
  - Significant performance benefit for wide tables

::: {.fragment}
```{r}
#| eval: false
system.time({
  df_subset <- read_parquet(
    "CA_person_2021.parquet", 
    col_select = c("PUMA", "COW")
  )
})
```
:::

::: {.columns}
::: {.column .fragment}
```
   user  system elapsed 
  0.027   0.003   0.031 
```
:::
::: {.column .fragment}
```
   user  system elapsed 
  1.017   0.207   0.568 
```
:::
:::
:::


## nanoparquet vs. arrow Reader

::: {.incremental}
- **nanoparquet**
  - Lightweight Parquet reader
  - Minimal dependencies
  - Good for embedding

- **arrow**
  - Full-featured reader
  - Support for datasets
  - Integration with Arrow ecosystem
  - Optimized for analytics workloads
:::

## nanoparquet vs. arrow Reader

```{r}
#| eval: false
library(arrow)
options(arrow.use_altrep = FALSE)

system.time({
  df <- read_parquet("CA_person_2021.parquet")
})
```

```
   user  system elapsed 
  1.017   0.207   0.568 
```

:::{.extrapad}
<br/>
:::

::: {.fragment}
```{r}
#| eval: false
library(nanoparquet)

system.time({
  df <- read_parquet("CA_person_2021.parquet")
})
```

```
   user  system elapsed 
  0.709   0.099   0.894 
```
:::



## Parquet Tooling Ecosystem

**Languages with native Parquet support:**

::: {.incremental}
- R (via arrow)
- Python (via pyarrow, pandas)
- Java
- C++
- Rust
- JavaScript
- Go
:::

## Parquet Tooling Ecosystem

**Systems with Parquet integration:**

::: {.incremental}
- Apache Spark
- Apache Hadoop
- Apache Drill
- Snowflake
- Amazon Athena
- Google BigQuery
- DuckDB
:::

# Working with Parquet files with Arrow in R

## Introduction to the arrow Package

```{r}
#| eval: false
# Install and load the Arrow package
install.packages("arrow")
library(arrow)

# Check Arrow version and capabilities
arrow_info()
```

::: {.incremental}
- The **arrow** package provides:
  - Native R interface to Apache Arrow
  - Tools for working with large datasets
  - Integration with dplyr for data manipulation
  - Reading/writing various file formats
:::

## Reading and Writing Parquet files, revisited

```{r}
#| eval: false
#| code-line-numbers: "|1-2|4-5|7-11|13-16"
# Read a Parquet file into R
data <- read_parquet("CA_person_2021.parquet")

# Write an R data frame to Parquet
write_parquet(data, "CA_person_2021_new.parquet")

# Reading a subset of columns
df_subset <- read_parquet(
  "CA_person_2021.parquet", 
  col_select = c("PUMA", "COW", "AGEP")
)

# Reading with a row filter (predicate pushdown)
df_filtered <- open_dataset("CA_person_2021.parquet") |> 
  filter(AGEP > 40) |>
  collect()
```

## Demo: Using dplyr with arrow

```{r}
#| eval: false
# Create an Arrow Table
df <- read_parquet("CA_person_2021.parquet", as_data_frame = FALSE)

# Use dplyr verbs with arrow tables
df |>
  filter(AGEP >= 16) |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) 
```

::: {.notes}
The dataframe is backed by altrep, actually. But generally functions like any other dataframe.
:::

# Querying Parquet with Different Engines

## Introduction to DuckDB

![](https://duckdb.org/images/duckdb-circle.svg){width=25%}

::: {.incremental}
- **Analytical SQL database system**
  - Embedded database (like SQLite)
  - Column oriented
  - In-process query execution

- **Key features:**
  - Direct Parquet querying
  - Vectorized query execution
  - Parallel processing
  - Zero-copy integration with arrow
:::

:::{.notes}
The zero-copy integration with arrow is because DuckDB uses basically the same format for it's own internal representation.
:::

## DuckDB

```{r}
#| eval: false
library(duckdb)

con <- dbConnect(duckdb())

# Register a Parquet file as a virtual table
dbExecute(con, "CREATE VIEW pums AS SELECT * 
                FROM read_parquet('CA_person_2021.parquet')")

# Run our query
dbGetQuery(con, "
  SELECT SUM(JWMNP * PWGTP)/SUM(PWGTP) as avg_commute_time,
         COUNT(*) as count
  FROM pums
  WHERE AGEP >= 16
")

dbDisconnect(con, shutdown = TRUE)
```

## duckplyr

```{r}
#| eval: false
library(duckplyr)

# Read data with Arrow
pums_data <- read_file_duckdb(
  "CA_person_2021.parquet", 
  "read_parquet"
)

# Use duckplyr to optimize dplyr operations
pums_data |>
  filter(AGEP >= 16) |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) |>
  collect()
```

:::{.notes}
duckplyr is a drop-in replacement for dplyr, using duckdb as a backend
:::

## data.table

```{r}
#| eval: false
library(arrow)
library(data.table)

# Read Parquet file with Arrow
pums_data <- read_parquet("CA_person_2021.parquet")

# Convert to data.table
pums_dt <- as.data.table(pums_data)

# data.table query
pums_dt[AGEP >= 16,
  .(avg_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP), 
    count = .N)]
```

## Arrow Query Execution

```{r}
#| eval: false
# Create an Arrow Dataset
pums_ds <- open_dataset("pums_dataset_dir/")

# Execute query with Arrow
result <- pums_ds |>
  filter(AGEP >= 16) |>
  group_by(ST) |>
  summarize(
    avg_commute_time = mean(JWMNP, na.rm = TRUE),
    count = n()
  ) |>
  arrange(desc(avg_commute_time)) |>
  collect()
```

## Demo: Seamless Integration Arrow ↔ DuckDB

```{r}
#| eval: false
df <- read_parquet("CA_person_2021.parquet")

# Use dplyr verbs with arrow tables
df |>
  filter(AGEP >= 16) |>
  to_duckdb() |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) 
```

# Arrow Datasets for Larger-than-Memory Operations

## Understanding Arrow Datasets vs. Tables

::: columns
::: {.column width="50%"}
**Arrow Table**

- In-memory data structure
- Must fit in RAM
- Fast operations
- Similar to base data frames
- Good for single file data
:::

::: {.column width="50%"}
**Arrow Dataset**

- Collection of files
- Lazily evaluated
- Larger-than-memory capable
- Distributed execution
- Supports partitioning
:::
:::

## Demo: Opening and Querying Multi-file Datasets

```{r}
#| eval: false
pums_ds <- open_dataset("data/person")

# Examine the dataset, list files
print(pums_ds)
head(pums_ds$files)

# Query execution with lazy evaluation
pums_ds |>
  filter(AGEP >= 16) |>
  group_by(year, ST) |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) |>
  collect()
```

## Lazy Evaluation and Query Optimization

::: {.incremental}
- **Lazy evaluation workflow:**
  1. Define operations (filter, group, summarize)
  2. Arrow builds an execution plan
  3. Optimizes the plan (predicate pushdown, etc.)
  4. Only reads necessary data from disk
  5. Executes when `collect()` is called

- **Benefits:**
  - Minimizes memory usage
  - Reduces I/O operations
  - Leverages Arrow's native compute functions
:::

## Working with Datasets on S3

arrow can work with data and datasets in cloud storage. This can be a good option if you don't have access to a formal DBMS.

::: {.incremental}
- Easy to store
- arrow efficiently uses metadata to read only what is necessary 
:::

:::{.notes}
I know, I know — this workshop is about **local** files. But I couldn't help myself
:::

## Demo: Working with Datasets on S3

```{r}
#| eval: false
pums_ds <- open_dataset("s3://scaling-arrow-pums/person/")

# Query execution with lazy evaluation
pums_ds |>
  filter(year == 2021, location == "ca", AGEP >= 16) |>
  group_by(year, ST) |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) |>
  collect()
```
:::{.notes}
Talk about partitioning helping, etc 
:::

## Demo: Sipping data

```{r}
#| eval: false
pums_ds <- open_dataset("s3://scaling-arrow-pums/person/")

# Query execution with lazy evaluation
pums_ds |>
  filter(AGEP >= 97) |>
  collect()
```

:::{.notes}
`Sys.getpid()`
`nettop -p X`
:::

# Partitioning Strategies

## What is Partitioning?

::: {.incremental}
- **Dividing data into logical segments**
  - Stored in separate files/directories
  - Based on one or more column values
  - Enables efficient filtering

- **Benefits:**
  - Faster queries that filter on partition columns
  - Improved parallel processing
  - Easier management of large datasets
:::

![](https://arrow.apache.org/docs/r/_images/dataset-parquet-partition.svg){width=50%}

## Hive vs. Non-Hive Partitioning

::: columns
::: {.column width="50%"}
**Hive Partitioning**

- Directory format: `column=value`
- Example:
  ```
  person/
  ├── year=2018/
  │   ├── state=NY/
  │   │   └── data.parquet
  │   └── state=CA/
  │       └── data.parquet
  ├── year=2019/
  │   ├── ...
  ```
- Self-describing structure
- Standard in big data ecosystem
:::

::: {.column width="50%"}
**Non-Hive Partitioning**

- Directory format: `value`
- Example:
  ```
  person/
  ├── 2018/
  │   ├── NY/
  │   │   └── data.parquet
  │   └── CA/
  │       └── data.parquet
  ├── 2019/
  │   ├── ...
  ```
- Requires column naming
- Less verbose directory names
:::
:::

## Effective Partitioning Strategies

::: {.incremental}
- **Choose partition columns wisely:**
  - Low to medium cardinality
  - Commonly used in filters
  - Balanced data distribution

- **Common partition dimensions:**
  - Time (year, month, day)
  - Geography (country, state, region)
  - Category (product type, department)
  - Source (system, sensor)
:::

## Partitioning in Practice: Writing Datasets

```{r}
#| eval: false
#| code-line-numbers: "|13-16"
ca_pums_data <- read_parquet("CA_person_2021.parquet")

ca_pums_data |>
  mutate(
    age_group = case_when(
      AGEP < 18 ~ "under_18",
      AGEP < 30 ~ "18_29",
      AGEP < 45 ~ "30_44",
      AGEP < 65 ~ "45_64",
      TRUE ~ "65_plus"
    )
  ) |>
  group_by(ST, age_group) |>
  write_dataset(
    path = "ca_pums_by_age/"
  )
```

## Demo: repartitioning the whole dataset
```{r}
#| eval: false
pums_data <- open_dataset("data/person")

pums_data |>
  mutate(
    age_group = case_when(
      AGEP < 18 ~ "under_18",
      AGEP < 30 ~ "18_29",
      AGEP < 45 ~ "30_44",
      AGEP < 65 ~ "45_64",
      TRUE ~ "65_plus"
    )
  ) |>
  group_by(year, ST, age_group) |>
  write_dataset(
    path = "pums_by_age/"
  )
```

## Best Practices for Partition Design

::: {.incremental}
- **Avoid over-partitioning:**
  - Too many small files = poor performance
  - Target file size: 20MB–2GB
  - Avoid high-cardinality columns (e.g., user_id)

- **Consider query patterns:**
  - Partition by commonly filtered columns
  - Balance between read speed and write complexity

- **Nested partitioning considerations:**
  - Order from highest to lowest selectivity
  - Limit partition depth (2-3 levels typically sufficient)
:::

## Partitioning Performance Impact

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 3
library(ggplot2)

data <- data.frame(
  Partitioning = c("No Partitioning", "Year Only", "Year+State", "Year+State+Age"),
  Time_Seconds = c(2.2, 1.0, 1.8, 6.5),
  Query = "Filter year >= 2018, mean commute time"
)

ggplot(data, aes(x = Partitioning, y = Time_Seconds, color = Partitioning)) +
  geom_point(stat = "identity", size = 3) +
  theme_minimal() +
  labs(x = "Partitioning Strategy", y = "Query Time (seconds)",
       title = "Impact of Partitioning on Query Performance") +
  theme(legend.position = "none")
```

```{r}
#| eval: false
open_dataset("<path/to/data>") |>
  filter(year >= 2018) |>
  summarise(
    mean_commute = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP)
  ) |>
  collect()
```


# Hands-on Workshop: Analysis with PUMS Data

## The PUMS Dataset

::: {.incremental}
- **Public Use Microdata Sample**
  - US Census Bureau data
  - Individual person and household records
  - Anonymized demographic information
  - Income, education, housing, commute, etc.

- **Dataset characteristics:**
  - Multiple years (2005-2022)
  - All US states and territories
  - 53 Million rows (person only)
  - 200+ variables
:::

## Reading Partitioned PUMS Data

```{r}
#| eval: false
library(arrow)
library(dplyr)

# Open the PUMS dataset
pums_path <- "data/person"  # Partitioned by year and location
pums_ds <- open_dataset(pums_path)

# Examine the dataset
pums_ds

# Look at the first few file paths
head(pums_ds$files)

# Examine schema
pums_ds$schema
```

## Exercise 1: Basic Filtering and Aggregation

```{r}
#| eval: false
# Calculate average income by state for 2021
result1 <- pums_ds |>
  filter(year == 2021) |>
  filter(PINCP > 0) |>  # Positive income only
  group_by(location) |>
  summarize(
    avg_income = mean(PINCP, na.rm = TRUE),
    median_income = quantile(PINCP, 0.5, na.rm = TRUE),
    n = n()
  ) |>
  arrange(desc(avg_income)) |>
  collect()

# View results
head(result1)
```

## Exercise 2: Using DuckDB with PUMS Data

```{r}
#| eval: false
library(arrow)
library(duckdb)
library(DBI)

# Open the PUMS dataset with Arrow
pums_ds <- open_dataset("data/person")

# Filter to just 2021 data for Washington
wa_2021 <- pums_ds |>
  filter(year == 2021, location == "wa") |>
  collect()

# Create DuckDB connection
con <- dbConnect(duckdb())

# Register Arrow Table with DuckDB
duckdb::duckdb_register_arrow(con, "pums_wa", wa_2021)

# Run SQL query
result <- dbGetQuery(con, "
  SELECT 
    CASE 
      WHEN AGEP < 18 THEN 'Under 18'
      WHEN AGEP < 30 THEN '18-29'
      WHEN AGEP < 45 THEN '30-44'
      WHEN AGEP < 65 THEN '45-64'
      ELSE '65+'
    END AS age_group,
    AVG(JWMNP) AS avg_commute_time,
    COUNT(*) AS n
  FROM pums_wa
  WHERE JWMNP > 0
  GROUP BY age_group
  ORDER BY age_group
")

# Disconnect when done
dbDisconnect(con, shutdown = TRUE)
```

## Challenge: Formulate Your Own Analysis

::: {.incremental}
- **Choose a research question:**
  - How has commute time changed over the years?
  - What's the relationship between education and income?
  - How does housing cost burden vary by state?
  - Your own question...

- **Implement the analysis using:**
  - Arrow Dataset operations
  - DuckDB SQL queries
  - Data visualization

- **Compare performance between approaches**
:::

## Conclusion

:::{.incremental}
- Column-oriented storage formats like Parquet provide massive 
performance advantages for analytical workloads (30x speed, 10x smaller
files)
- Apache Arrow enables seamless data interchange between systems without
costly serialization/deserialization
- Multiple query engines (arrow, DuckDB, data.table) offer flexibility
depending on your analysis needs, all using modern formats like Parquet
- Partitioning strategies help manage large datasets effectively when
working with data too big for memory
:::

## Conclusion

**Resources:**

- Workshop materials: [GitHub Repository](https://github.com/your-repo)
- Arrow documentation: [arrow.apache.org/docs/r](https://arrow.apache.org/docs/r/)
- Parquet: [parquet.apache.org](https://parquet.apache.org/)
- DuckDB: [duckdb.org](https://duckdb.org/)
- Book: [Scaling up with Arrow and R](https://arrow-user2022.github.io/scaling-r-with-arrow/)

**Questions?**
