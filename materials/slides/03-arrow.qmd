---
title: "parquet, arrow, duckdb"
execute:
  echo: true
format: 
  revealjs:
    footer: "[JSM: Large Data](https://github.com/kbodwin/jsm-big-data-2025/tree/main/materials)"
    theme: simple
    # scrollable: true
    embed-resources: true
    include-in-header:
      - text: |
          <style>
          .extrapad {
              margin-top:1em  !important;
          }
          .v-spacier {
              margin-bottom: 2em; 
              margin-top: 2em;
          }
          </style>
editor: source
---

## Section Overview

1. **Introduction to Column-Oriented Data Storage**
   <!-- - The big data interchange problem
   - Row vs. column storage formats
   - Introduction to Parquet and Arrow -->

2. **Deep Dive into Parquet** 
   <!-- - Key features and benefits
   - Performance benchmarks
   - Demo: Efficient dataset storage -->

3. **Working with Arrow in R** 
   <!-- - The Arrow package
   - Reading/writing Parquet files
   - Using dplyr with Arrow tables -->

4. **Querying Parquet with Different Engines** 
   <!-- - DuckDB: SQL queries on Parquet files
   - Data.table approach
   - Arrow query execution
   - Performance comparisons -->

5. **Arrow Datasets for Larger-than-Memory Operations** 
   <!-- - Datasets vs. Tables
   - Handling data too large for memory
   - Working with datasets on S3 -->

6. **Partitioning Strategies** 
   <!-- - Understanding partitioning
   - Hive vs. non-Hive partitioning
   - Best practices -->

7. **Hands-on Workshop: Analysis with PUMS Data** 
   <!-- - Reading partitioned Parquet files
   - Executing queries with DuckDB and Arrow
   - Practice exercises -->

# Introduction to Column-Oriented Data Storage

## Why should I care about data storage?

::: {.fragment .v-spacier}
Data has to be represented somewhere, both during analysis and when storing.
:::

::: {.fragment .v-spacier}
The shape and characteristics of this representation has a huge impact on performance.
:::

::: {.fragment .v-spacier}
What if you could speed up a key part of your analysis by 30x and reduce your storage by 10x?
:::

## Row vs. Column-Oriented Storage

::: columns
::: {.column width="50%"}
**Row-oriented**

```
|ID|Name |Age|City    |
|--|-----|---|--------|
|1 |Alice|25 |New York|
|2 |Bob  |30 |Boston  |
|3 |Carol|45 |Chicago |
```

::: {.extrapad}
- Efficient for single record access
- Efficient for appending
:::
:::

::: {.column width="50%"}
::: {.fragment}
**Column-oriented**

```
ID:    [1, 2, 3]
Name:  [Alice, Bob, Carol]
Age:   [25, 30, 45]
City:  [New York, Boston, Chicago]
```

::: {.extrapad}
- Efficient for analytics
- Better compression
:::
:::
:::
:::

::: {.notes}
Row oriented formats are super familiar: CSVs as well as many databases

But Column-orientation isn't something that is new and cutting edge. In fact, every single one of you use a system that stores data this way: R data frames(!)
:::

## Why Column-Oriented Storage?

::: {.incremental}
- **Analytics typically access a subset of columns**
  - "What is the average age by city?"
  - Only needs [Age, City] columns

- **Benefits:**
  - Only read needed columns from disk
  - Similar data types stored together
  - Better compression ratios
:::

::: {.notes}
Compression: this is because like-types are stored with like, so you get more frequent patterns — the core of compression. But you also can use encodings like dictionary encodings very efficiently.
:::

## Column-Oriented Data is great 

:::{.extrapad}
And you use column-oriented dataframes already!
:::

::: {.fragment .extrapad}
... but still storing my data in a fundamentally row-oriented way. 
:::

::: {.notes}
This isn't so bad if you're only talking about a small amount of data, transposing a few columns for a few rows is no big deal. But as data gets larger, or if you have to do this frequently, this process of transposing (AKA serialization) hurts.
:::

## The serialization problem

![](images/intro_copy_and_convert.png){.r-stretch}

::: {.notes}
Many of these were operating in essentially column-oriented ways — but to transfer data ended up writting into row-oriented data structures, then read them back in to something that was column-oriented.

**Moving data between representations is hard**
  - Different formats, requirements, and limitations
  - Compatibility issues
  - Serialization is a huge bottleneck
:::

## The serialization problem

![](images/intro_arrow_adapter.png){.r-stretch}

## What is Apache Arrow?

![](images/arrow-logo.png){.absolute top=0 right=0 width="200"}

::: {.incremental}
- **Cross-language development platform for**   
    **in-memory data**
  - Consistent in-memory columnar data format
  - Language-independent
  - Zero-copy reads
:::

## What is Apache Arrow?

![](images/arrow-logo.png){.absolute top=0 right=0 width="200"}

::: {.incremental}
- **Benefits:**
  - Seamless data interchange between systems
  - Fast analytical processing
  - Efficient memory usage
:::

## What is Apache Parquet?

![](images/Apache_Parquet_logo.svg){.absolute top=0 right=0 width="400"}

::: {.incremental}
- **Open-source columnar storage format**
  - Created by Twitter and Cloudera in 2013
  - Part of the Apache Software Foundation
:::

## What is Apache Parquet?

![](images/Apache_Parquet_logo.svg){.absolute top=0 right=0 width="400"}

::: {.incremental}
- **Features:**
  - Columnar storage
  - Explicit schema 
  - Statistical metadata
  - Efficient compression
:::

## Reading a File

As a CSV file

```{r}
#| eval: false
system.time({
  df <- read.csv("CA_person_2021.csv")
})
```
:::{.fragment .extrapad}
```
   user  system elapsed 
 14.449   0.445  15.037 
```
:::

:::{.notes}
Describe the CSV

This CSV is 708 MB, I'm reading this in on my MacBook Pro, your times will vary! We can use arrow or data.tables's CSV reader and it's faster (1.85 sec and 1.61 sec respectively). And if we read to an arrow table it's even faster: 0.51 seconds
:::

## Reading a File

As a Parquet file

```{r}
#| eval: false
#| code-line-numbers: '|2|'
library(arrow)
options(arrow.use_altrep = FALSE)

system.time({
  df <- read_parquet("CA_person_2021.parquet")
})
```
:::{.fragment .extrapad}
```
   user  system elapsed 
  1.017   0.207   0.568 
```
:::

:::{.notes}
The parquet file is 62 MB

It's even faster with altrep (0.186 s), but that's cheating! Also, if we read into an arrow table rather than a dataframe: 0.1 second 
:::


## Exercise

```{r}
#| eval: false
data <- tibble::tibble(
  integers = 1:10,
  doubles = as.numeric(1:10),
  strings = sprintf("%02d", 1:10)
)

write.csv(data, "numeric_base.csv", row.names = FALSE)
write_csv_arrow(data, "numeric_arrow.csv")
write_parquet(data, "numeric.parquet")

df_csv <- read.csv("numeric_base.csv")
df_csv_arrow <- read_csv_arrow("numeric_arrow.csv")
df_parquet <- read_parquet("numeric.parquet")
```

::: {.fragment .extrapad}
Are there any differences?
:::

## Exercise (answer)

:::{.columns}
:::{.column}
```{.default}
> df_csv_arrow
# A tibble: 10 × 3
   integers doubles strings
      <int>   <int>   <int>
 1        1       1       1
 2        2       2       2
 3        3       3       3
 4        4       4       4
 5        5       5       5
 6        6       6       6
 7        7       7       7
 8        8       8       8
 9        9       9       9
10       10      10      10
```
:::

:::{.column}
```{.default}
> df_parquet
# A tibble: 10 × 3
   integers doubles strings
      <int>   <dbl> <chr>  
 1        1       1 01     
 2        2       2 02     
 3        3       3 03     
 4        4       4 04     
 5        5       5 05     
 6        6       6 06     
 7        7       7 07     
 8        8       8 08     
 9        9       9 09     
10       10      10 10     
```
:::
:::

## Exercise (answer)

:::{.columns}
:::{.column}
```{.default code-line-numbers="3-4"}
> df_csv_arrow
# A tibble: 10 × 3
   integers doubles strings
      <int>   <int>   <int>
 1        1       1       1
 2        2       2       2
 3        3       3       3
 4        4       4       4
 5        5       5       5
 6        6       6       6
 7        7       7       7
 8        8       8       8
 9        9       9       9
10       10      10      10
```
:::

:::{.column}
```{.default code-line-numbers="3-4"}
> df_parquet
# A tibble: 10 × 3
   integers doubles strings
      <int>   <dbl> <chr>  
 1        1       1 01     
 2        2       2 02     
 3        3       3 03     
 4        4       4 04     
 5        5       5 05     
 6        6       6 06     
 7        7       7 07     
 8        8       8 08     
 9        9       9 09     
10       10      10 10     
```
:::
:::

# Deep Dive into Parquet

## What is inside a Parquet file?

::: {.incremental}
- **Schema metadata**
  - Self-describing format
  - Preserves column types
  - Type-safe data interchange

- **The data itself**
  - Encodings
  - Advanced compression
:::

:::{.notes}
Encodings:
  - **Dictionary** — Particularly effective for categorical data
  - **Run-length encoding** - Efficient storage of sequential repeated values

Advanced compression:   

  - Column-specific compression algorithms
  - Both dictionary and value compression
:::

## Structure of a Parquet File

![](images/files_formats_parquet_bw.png){width=100%}

::: {.notes}
- **Row groups**: Horizontal partitions of data
- **Column chunks**: Columnar data within a row group
- **Pages**: Small units of column chunk data
- **Footer**: Contains file metadata and schema
:::

## Benchmarks: Parquet vs CSV

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 4.25
#| fig-height: 5
library(ggplot2)
data <- data.frame(
  Format = c("CSV", "CSV (compressed)", "Parquet"),
  Size_MB = c(707.9, 101.6, 62)
)
ggplot(data, aes(x = Format, y = Size_MB, fill = Format)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(y = "Size (MB)", title = "File Size Comparison") + 
  guides(fill="none")
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 4.25
#| fig-height: 5
data <- data.frame(
  Format = c("CSV", "CSV (arrow)", "CSV (compressed)", "Parquet"),
  Time_Seconds = c(15.037, 1.806, 1.865,  0.568)
)
ggplot(data, aes(x = Format, y = Time_Seconds, color = Format)) +
  geom_point(stat = "identity", size = 3) +
  theme_minimal() +
  labs(y = "Time (seconds)", title = "Read Performance") + 
  guides(color="none")
```
:::
:::

## Benchmarks: Parquet vs CSV

```{r}
#| echo: false
#| fig-width: 7.5
#| fig-height: 3
data <- data.frame(
  Format = c("CSV (arrow)", "CSV (arrow, compressed)", "CSV (dt)", "CSV (dt, compressed)", "Parquet"),
  Time_Seconds = c(1.806, 1.865, 1.851, 2.325, 0.568)
)
ggplot(data, aes(x = Format, y = Time_Seconds, color = Format)) +
  geom_point(stat = "identity", size = 3) +
  theme_minimal() +
  labs(y = "Time (seconds)", title = "Read Performance, modern packages") + 
  guides(color="none")
```

## Reading Efficiency: Selecting Columns

::: {.incremental}
- **With CSV:**
  - Must read entire file, even if you only need a few columns
  - No efficient way to skip columns during read

- **With Parquet:**
  - Read only needed columns from disk
  - Significant performance benefit for wide tables
:::

## Reading Efficiency: Selecting Columns

::: {.incremental}
::: {.fragment}
```{r}
#| eval: false
system.time({
  df_subset <- read_parquet(
    "CA_person_2021.parquet", 
    col_select = c("PUMA", "COW")
  )
})
```
:::

::: {.columns}
::: {.column .fragment}
```
   user  system elapsed 
  0.027   0.003   0.031 
```
:::
::: {.column .fragment}
```
   user  system elapsed 
  1.017   0.207   0.568 
```
:::
:::
:::


## nanoparquet vs. arrow Reader

::: {.incremental}
- **nanoparquet**
  - Lightweight Parquet reader
  - Minimal dependencies
  - Good for embedding

- **arrow**
  - Full-featured reader
  - Support for datasets
  - Integration with Arrow ecosystem
:::

## nanoparquet vs. arrow Reader

```{r}
#| eval: false
library(arrow)
options(arrow.use_altrep = FALSE)

system.time({
  df <- read_parquet("CA_person_2021.parquet")
})
```

```
   user  system elapsed 
  1.017   0.207   0.568 
```

:::{.extrapad}
<br/>
:::

::: {.fragment}
```{r}
#| eval: false
library(nanoparquet)

system.time({
  df <- read_parquet("CA_person_2021.parquet")
})
```

```
   user  system elapsed 
  0.709   0.099   0.894 
```
:::



## Parquet Tooling Ecosystem

**Languages with native Parquet support:**

::: {.incremental}
- R (via arrow, nanoparquet)
- Python (via pyarrow, pandas)
- Java
- C++
- Rust
- JavaScript
- Go
:::

## Parquet Tooling Ecosystem

**Systems with Parquet integration:**

::: {.incremental}
- DuckDB
- Google BigQuery
- Snowflake
- Amazon Athena
- Apache Spark
- Apache Hadoop
:::

# Working with Parquet files with Arrow in R

## Introduction to the arrow Package

```{r}
#| eval: false
# Install and load the Arrow package
install.packages("arrow")
library(arrow)

# Check Arrow version and capabilities
arrow_info()
```

::: {.incremental}
- The **arrow** package provides:
  - Native R interface to Apache Arrow
  - Tools for working with large datasets
  - Integration with dplyr for data manipulation
  - Reading/writing various file formats
:::

## Reading and Writing Parquet files, revisited

```{r}
#| eval: false
#| code-line-numbers: "|1-2|4-5|7-11|13-16"
# Read a Parquet file into R
data <- read_parquet("CA_person_2021.parquet")

# Write an R data frame to Parquet
write_parquet(data, "CA_person_2021_new.parquet")

# Reading a subset of columns
df_subset <- read_parquet(
  "CA_person_2021.parquet", 
  col_select = c("PUMA", "COW", "AGEP")
)

# Reading with a row filter (predicate pushdown)
df_filtered <- open_dataset("CA_person_2021.parquet") |> 
  filter(AGEP > 40) |>
  collect()
```

## Demo: Using dplyr with arrow

```{r}
#| eval: false
# Create an Arrow Table
table <- read_parquet("CA_person_2021.parquet", as_data_frame = FALSE)

# Use dplyr verbs with arrow tables
table |>
  filter(AGEP >= 16) |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) |>
  collect()
```

::: {.notes}
Without as_data_frame = FALSE: the dataframe is backed by altrep, actually. But generally functions like any other dataframe.
:::

# Querying Parquet with Different Engines

## Introduction to DuckDB

![](https://duckdb.org/images/duckdb-circle.svg){.absolute top=0 right=0 width="200"}

::: {.incremental}
- **Analytical SQL database system**
  - Embedded database (like SQLite)
  - Column oriented
  - In-process query execution

- **Features:**
  - Direct Parquet querying
  - Parallel processing
  - Zero-copy integration with arrow
:::

:::{.notes}
The zero-copy integration with arrow is because DuckDB uses basically the same format for it's own internal representation.
:::

## DuckDB

```{r}
#| eval: false
#| code-line-numbers: "|9-15"
library(duckdb)

con <- dbConnect(duckdb())

# Register a Parquet file as a virtual table
dbExecute(con, "CREATE VIEW pums AS SELECT * 
                FROM read_parquet('CA_person_2021.parquet')")

# Run our query
dbGetQuery(con, "
  SELECT SUM(JWMNP * PWGTP)/SUM(PWGTP) as avg_commute_time,
         COUNT(*) as count
  FROM pums
  WHERE AGEP >= 16
")

dbDisconnect(con, shutdown = TRUE)
```

## duckplyr

```{r}
#| eval: false
#| code-line-numbers: "|9-17"
library(duckplyr)

# Read data with Arrow
pums_data <- read_file_duckdb(
  "CA_person_2021.parquet", 
  "read_parquet"
)

# Use duckplyr to optimize dplyr operations
pums_data |>
  filter(AGEP >= 16) |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) |>
  collect()
```

:::{.notes}
duckplyr is a drop-in replacement for dplyr, using duckdb as a backend
:::

## data.table

```{r}
#| eval: false
#| code-line-numbers: "|10-13"
library(arrow)
library(data.table)

# Read Parquet file with Arrow
pums_data <- read_parquet("CA_person_2021.parquet")

# Convert to data.table
pums_dt <- as.data.table(pums_data)

# data.table query
pums_dt[AGEP >= 16,
  .(avg_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP), 
    count = .N)]
```

## Demo: Seamless Integration Arrow ↔ DuckDB

```{r}
#| eval: false
#| code-line-numbers: "|6"
table <- read_parquet("CA_person_2021.parquet", as_data_frame = FALSE)

# Use dplyr verbs with arrow tables
table |>
  filter(AGEP >= 16) |>
  to_duckdb() |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) 
```

# Arrow Datasets for Larger-than-Memory Operations

## Understanding Arrow Datasets vs. Tables

::: columns
::: {.fragment .column width="50%"}
**Arrow Table**

- In-memory data structure
- Must fit in RAM
- Fast operations
- Similar to base data frames
- Good for single file data
:::

::: {.fragment .column width="50%"}
**Arrow Dataset**

- Collection of files
- Lazily evaluated
- Larger-than-memory capable
- Distributed execution
- Supports partitioning
:::
:::

## Demo: Querying Multi-file Datasets

```{r}
#| eval: false
pums_ds <- open_dataset("data/person")

# Examine the dataset, list files
print(pums_ds)
head(pums_ds$files)

# Query execution with lazy evaluation
pums_ds |>
  filter(AGEP >= 16) |>
  group_by(year, ST) |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) |>
  collect()
```

:::{.notes}
Mention Acero engine in C++
:::

## Lazy Evaluation and Query Optimization

::: {.incremental}
- **Lazy evaluation workflow:**
  - Define operations (filter, group, summarize)
  - Optimizes the plan (predicate pushdown, et c.)
  - Only reads necessary data from disk
  - Executes when `collect()` is called

- **Benefits:**
  - Minimizes memory usage + reduces I/O 
  - Leverages Arrow's native compute functions
:::

## Working with Datasets on S3

arrow can work with data and datasets in cloud storage. This can be a good option if you don't have access to a formal DBMS.

::: {.incremental}
- Easy to store
- arrow efficiently uses metadata to read only what is necessary 
:::

:::{.notes}
I know, I know — this workshop is about **local** files. But I couldn't help myself
:::

## Demo: Working with Datasets on S3

```{r}
#| eval: false
pums_ds <- open_dataset("s3://scaling-arrow-pums/person/")

# Query execution with lazy evaluation
pums_ds |>
  filter(year == 2021, location == "ca", AGEP >= 16) |>
  group_by(year, ST) |>
  summarize(
    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /
      sum(PWGTP),
    count = n()
  ) |>
  collect()
```
:::{.notes}
Talk about partitioning helping, etc 
:::

## Demo: Sipping data

```{r}
#| eval: false
pums_ds <- open_dataset("s3://scaling-arrow-pums/person/")

# Query execution with lazy evaluation
pums_ds |>
  filter(AGEP >= 97) |>
  collect()
```

:::{.notes}
Around ~110MB

`Sys.getpid()`
`nettop -p X`
:::

# Partitioning Strategies

## What is Partitioning?

::: {.incremental}
- **Dividing data into logical segments**
  - Stored in separate files/directories
  - Based on one or more column values
  - Enables efficient filtering

- **Benefits:**
  - Faster queries that filter on partition columns
  - Improved parallel processing
  - Easier management of large datasets
:::

## Hive vs. Non-Hive Partitioning {.smaller}

::: columns 
::: {.column width="50%"}
**Hive Partitioning**

- Directory format: `column=value`
- Example:
  ```
  person/
  ├── year=2018/
  │   ├── state=NY/
  │   │   └── data.parquet
  │   └── state=CA/
  │       └── data.parquet
  ├── year=2019/
  │   ├── ...
  ```
- Self-describing structure
- Standard in big data ecosystem
:::

::: {.column width="50%"}
**Non-Hive Partitioning**

- Directory format: `value`
- Example:
  ```
  person/
  ├── 2018/
  │   ├── NY/
  │   │   └── data.parquet
  │   └── CA/
  │       └── data.parquet
  ├── 2019/
  │   ├── ...
  ```
- Requires column naming
- Less verbose directory names
:::
:::

## Effective Partitioning Strategies

::: {.incremental}
- **Choose partition columns wisely:**
  - Commonly used in filters
  - Low to medium cardinality

- **Common partition dimensions:**
  - Time (year, month, day)
  - Geography (country, state, region)
  - Category (product type, department)
:::

## Partitioning in Practice: Writing Datasets

```{r}
#| eval: false
#| code-line-numbers: "|13-16"
ca_pums_data <- read_parquet("CA_person_2021.parquet")

ca_pums_data |>
  mutate(
    age_group = case_when(
      AGEP < 18 ~ "under_18",
      AGEP < 30 ~ "18_29",
      AGEP < 45 ~ "30_44",
      AGEP < 65 ~ "45_64",
      TRUE ~ "65_plus"
    )
  ) |>
  group_by(ST, age_group) |>
  write_dataset(
    path = "ca_pums_by_age/"
  )
```

## Demo: Repartitioning the whole dataset
```{r}
#| eval: false
pums_data <- open_dataset("data/person")

pums_data |>
  mutate(
    age_group = case_when(
      AGEP < 18 ~ "under_18",
      AGEP < 30 ~ "18_29",
      AGEP < 45 ~ "30_44",
      AGEP < 65 ~ "45_64",
      TRUE ~ "65_plus"
    )
  ) |>
  group_by(year, ST, age_group) |>
  write_dataset(
    path = "pums_by_age/"
  )
```

:::{.notes}
   user  system elapsed 
386.530  65.862 115.496 
:::

## Best Practices for Partition Design

::: {.incremental}
- **Avoid over-partitioning:**
  - Too many small files = poor performance
  - Target file size: 20MB–2GB
  - Avoid high-cardinality columns (e.g., user_id)

- **Consider query patterns:**
  - Partition by commonly filtered columns
  - Balance between read speed and write complexity
:::

## Partitioning Performance Impact

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 2.75
library(ggplot2)

data <- data.frame(
  Partitioning = c("No Partitioning", "Year Only", "Year+State", "Year+State+Age"),
  Time_Seconds = c(2.2, 1.0, 1.8, 6.5),
  Query = "Filter year >= 2018, mean commute time"
)

ggplot(data, aes(x = Partitioning, y = Time_Seconds, color = Partitioning)) +
  geom_point(stat = "identity", size = 3) +
  theme_minimal() +
  labs(x = "Partitioning Strategy", y = "Query Time (seconds)",
       title = "Impact of Partitioning on Query Performance") +
  theme(legend.position = "none")
```

```{r}
#| eval: false
open_dataset("<path/to/data>") |>
  filter(year >= 2018) |>
  summarise(
    mean_commute = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP)
  ) |>
  collect()
```

# [Hands-on practice: Analysis with PUMS Data](../activities/03-practice-arrow.qmd)

## Conclusion

:::{.incremental}
- Column-oriented storage formats like Parquet provide massive 
performance advantages for analytical workloads (30x speed, 10x smaller
files)
- Partitioning strategies help manage large datasets effectively when
working with data too big for memory
:::

## Conclusion

:::{.incremental}
- Apache Arrow enables seamless data interchange between systems without
costly serialization/deserialization
- Multiple query engines (arrow, DuckDB, data.table) offer flexibility
depending on your analysis needs, all using modern formats like Parquet
:::

## Conclusion

**Resources:**

- Workshop materials: [GitHub Repository](https://github.com/kbodwin/jsm-big-data-2025)
- Arrow documentation: [arrow.apache.org/docs/r](https://arrow.apache.org/docs/r/)
- Parquet: [parquet.apache.org](https://parquet.apache.org/)
- DuckDB: [duckdb.org](https://duckdb.org/)
- Book: [Scaling up with Arrow and R](https://arrowrbook.com/)    
    20% Discount on a [physical book](https://www.routledge.com/) with code: **25AFLY2**
    