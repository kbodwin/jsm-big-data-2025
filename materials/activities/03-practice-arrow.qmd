---
title: "Hands-on practice: Using parquet, arrow, and duckdb"
editor: source
---

## Exercise 1: Reading Partitioned PUMS Data

::: panel-tabset
### Problems

* Load the PUMS person data as an arrow dataset
* What are the dimensions of the dataset? Number of rows? Number of columns?
* Examine the dataset:
  * What columns does the dataset have?
  * What are the column types?

### Hints

You can use `arrow::open_dataset(...)` to open a dataset. Treat the dataset like it's a data frame, can you use the same functions that you are already familiar with?

### Solution

```{r}
#| eval: false
library(arrow)
library(dplyr)

# Open the PUMS dataset
pums_ds <- open_dataset("~/PUMS/data/person")

# dimensions
nrow(pums_ds)
ncol(pums_ds)

# Examine the dataset
pums_ds

# Examine schema
schema(pums_ds)
```
:::

## Exercise 2: Basic Filtering and Aggregation

::: panel-tabset
### Problems

* What was the population of California in 2018?
* What was the population of California over the last five years of available data?

### Hint

Try using familiar dplyr verbs: `filter()`, `summarize()`, et c.

### Solution

```{r}
#| eval: false
library(arrow)
library(dplyr)

# Open the PUMS dataset
pums_ds <- open_dataset("~/PUMS/data/person")

pums_ds |>
  filter(year == 2018, location == "ca") |>
  summarize(
    n = n(),
    population = sum(PWGTP)
  ) |>
  collect()

pums_ds |>
  filter(location == "ca") |>
  group_by(year) |>
  summarize(
    n = n(),
    population = sum(PWGTP)
  ) |>
  arrange(-year) |>
  slice_head(n = 5) |>
  collect()
```
:::

## Exercise 3: Using DuckDB with PUMS Data

::: panel-tabset
### Problems

For each state, compute the minimum commute time and the maximum commute time. Return it in a long data frame (hint, you'll need to use `tidyr::pivot_longer` here) with the columns: `location`, `commute_metric` (values: `max_commute`, `min_commute`), `time`. 

For motivation: if you were trying to plot using `ggplot2` and wanted both the minimums and the maximums to show up on the same plot.

### Hint
Arrow's query engine doesn't support `tidyr::pivot_longer`. So you'll need to use `to_duckdb()` and then use duckdb from there.

### Solution
```{r}
#| eval: false
library(arrow)
library(duckdb)
library(tidyr)

# Open the PUMS dataset
pums_ds <- open_dataset("~/PUMS/data/person")

pums_ds |>
  group_by(location) |>
  summarize(
    max_commute = max(JWMNP, na.rm = TRUE),
    min_commute = min(JWMNP, na.rm = TRUE)
  ) |>
  to_duckdb() |> 
  pivot_longer(!location, names_to = "commute_metric", values_to = "time") |>
  arrange(location) |>
  collect()
```
:::

## Challenge: Formulate Your Own Analysis

- **Choose a research question:**
  - How has commute time changed over the years?
  - What's the relationship between education and income?
  - How does housing cost burden vary by state?
  - Your own question...

- **Implement the analysis using:**
  - Arrow Dataset operations
  - DuckDB SQL queries
  - Data visualization

- **Compare performance between approaches**


## The PUMS Dataset

[detailed dataset description](https://scaling-arrow-pums.s3.us-east-1.amazonaws.com/readme.html)

- **Public Use Microdata Sample**
  - US Census Bureau data
  - Individual person and household records
  - Anonymized demographic information
  - Income, education, housing, commute, et c.

- **Dataset characteristics:**
  - Multiple years (2005-2022, sans 2020)
  - All US states and territories
  - 53 Million rows (person), 25 Million rows (household)
  - 200+ variables