[
  {
    "objectID": "materials/activities/01-practice-data.html#demo",
    "href": "materials/activities/01-practice-data.html#demo",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "materials/activities/01-practice-data.html#your-turn",
    "href": "materials/activities/01-practice-data.html#your-turn",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Your Turn",
    "text": "Your Turn"
  },
  {
    "objectID": "materials/activities/02-practice-datatable.html#demo",
    "href": "materials/activities/02-practice-datatable.html#demo",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "materials/activities/02-practice-datatable.html#your-turn",
    "href": "materials/activities/02-practice-datatable.html#your-turn",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Your Turn",
    "text": "Your Turn"
  },
  {
    "objectID": "materials/slides/05-PUMS-dataset.html#about-the-data",
    "href": "materials/slides/05-PUMS-dataset.html#about-the-data",
    "title": "Introduction to PUMS",
    "section": "About the data",
    "text": "About the data\n\nCollected by the United States Census Bureau as part of the American Community Survey\nDisclosure protection ‚Äî introduces noise to make it impossible to identify specific people or households\nCovers: 2005‚Äì2022 using the 1-year estimates (sans 2020; COVID)\nSplit into person and household\n\ncolumns: person: 230, household: 188\nrows: person: 53M, household: 25M"
  },
  {
    "objectID": "materials/slides/05-PUMS-dataset.html#a-few-example-variables",
    "href": "materials/slides/05-PUMS-dataset.html#a-few-example-variables",
    "title": "Introduction to PUMS",
    "section": "A few example variables",
    "text": "A few example variables\n\nPerson\n\nLanguage spoken at home (LANP)\nTravel time to work (JWMNP)\n\nHousehold\n\nAccess to internat (ACCESS)\nMonthly rent (RNTP)\n\nWeights üòµ‚Äçüí´\n\nPWGTP and WGTP for weights"
  },
  {
    "objectID": "materials/slides/05-PUMS-dataset.html#format-of-the-data",
    "href": "materials/slides/05-PUMS-dataset.html#format-of-the-data",
    "title": "Introduction to PUMS",
    "section": "Format of the data",
    "text": "Format of the data\n\nReleased and available as CSV files (~90GB)\nUses survey-style coding\n\n\nFor this workshop:\n\nRecoded the dataset\nSaved as parquet (~12GB) partitioned by year and state\n\n\nsurvey-style coding is where categorical variables will be given a number and there is a separate look up table for the values. Additionally, there are frequently sentinel values that mean missing or ‚Äú99 and greater‚Äù\nWe have pulled the key into the actual data so you don‚Äôt need to do the lookups and also converted numeric columns into integers, floats, et c.¬†where appropriate."
  },
  {
    "objectID": "materials/slides/05-PUMS-dataset.html#can-i-analyze-all-of-pums",
    "href": "materials/slides/05-PUMS-dataset.html#can-i-analyze-all-of-pums",
    "title": "Introduction to PUMS",
    "section": "Can I analyze all of PUMS?",
    "text": "Can I analyze all of PUMS?\n\nMost analysis of PUMS data starts with subsetting the data. Either by state (or even smaller) or year and often both.\n\n\nBut with the tools we learn about in this workshop, we actually can analyze the whole dataset."
  },
  {
    "objectID": "materials/slides/05-PUMS-dataset.html#what-can-i-do",
    "href": "materials/slides/05-PUMS-dataset.html#what-can-i-do",
    "title": "Introduction to PUMS",
    "section": "What can I do?",
    "text": "What can I do?"
  },
  {
    "objectID": "materials/slides/05-PUMS-dataset.html#what-can-i-do-1",
    "href": "materials/slides/05-PUMS-dataset.html#what-can-i-do-1",
    "title": "Introduction to PUMS",
    "section": "What can I do?",
    "text": "What can I do?"
  },
  {
    "objectID": "materials/slides/05-PUMS-dataset.html#caveat",
    "href": "materials/slides/05-PUMS-dataset.html#caveat",
    "title": "Introduction to PUMS",
    "section": "Caveat",
    "text": "Caveat\n\nThough we have not purposefully altered this data, this data should not be relied on to be a perfect or even possibly accurate representation of the official PUMS dataset."
  },
  {
    "objectID": "materials/slides/03-arrow.html#section-overview",
    "href": "materials/slides/03-arrow.html#section-overview",
    "title": "parquet, arrow, duckdb",
    "section": "Section Overview",
    "text": "Section Overview\n\nIntroduction to Column-Oriented Data Storage \nDeep Dive into Parquet \nWorking with Arrow in R \nQuerying Parquet with Different Engines \nArrow Datasets for Larger-than-Memory Operations \nPartitioning Strategies \nHands-on Workshop: Analysis with PUMS Data"
  },
  {
    "objectID": "materials/slides/03-arrow.html#why-should-i-care-about-data-storage",
    "href": "materials/slides/03-arrow.html#why-should-i-care-about-data-storage",
    "title": "parquet, arrow, duckdb",
    "section": "Why should I care about data storage?",
    "text": "Why should I care about data storage?\n\nData has to be represented somewhere, both during analysis and when storing.\n\n\nThe shape and characteristics of this representation has a huge impact on performance.\n\n\nWhat if you could speed up a key part of your analysis by 30x and reduce your storage by 10x?"
  },
  {
    "objectID": "materials/slides/03-arrow.html#row-vs.-column-oriented-storage",
    "href": "materials/slides/03-arrow.html#row-vs.-column-oriented-storage",
    "title": "parquet, arrow, duckdb",
    "section": "Row vs.¬†Column-Oriented Storage",
    "text": "Row vs.¬†Column-Oriented Storage\n\n\nRow-oriented\n|ID|Name |Age|City    |\n|--|-----|---|--------|\n|1 |Alice|25 |New York|\n|2 |Bob  |30 |Boston  |\n|3 |Carol|45 |Chicago |\n\n\nEfficient for single record access\nEfficient for appending\n\n\n\n\nColumn-oriented\nID:    [1, 2, 3]\nName:  [Alice, Bob, Carol]\nAge:   [25, 30, 45]\nCity:  [New York, Boston, Chicago]\n\n\nEfficient for analytics\nBetter compression\n\n\n\n\n\nRow oriented formats are super familiar: CSVs as well as many databases\nBut Column-orientation isn‚Äôt something that is new and cutting edge. In fact, every single one of you use a system that stores data this way: R data frames(!)"
  },
  {
    "objectID": "materials/slides/03-arrow.html#why-column-oriented-storage",
    "href": "materials/slides/03-arrow.html#why-column-oriented-storage",
    "title": "parquet, arrow, duckdb",
    "section": "Why Column-Oriented Storage?",
    "text": "Why Column-Oriented Storage?\n\nAnalytics typically access a subset of columns\n\n‚ÄúWhat is the average age by city?‚Äù\nOnly needs [Age, City] columns\n\nBenefits:\n\nOnly read needed columns from disk\nSimilar data types stored together\nBetter compression ratios\n\n\n\nCompression: this is because like-types are stored with like, so you get more frequent patterns ‚Äî the core of compression. But you also can use encodings like dictionary encodings very efficiently."
  },
  {
    "objectID": "materials/slides/03-arrow.html#column-oriented-data-is-great",
    "href": "materials/slides/03-arrow.html#column-oriented-data-is-great",
    "title": "parquet, arrow, duckdb",
    "section": "Column-Oriented Data is great",
    "text": "Column-Oriented Data is great\n\nAnd you use column-oriented dataframes already!\n\n\n‚Ä¶ but still storing my data in a fundamentally row-oriented way.\n\n\nThis isn‚Äôt so bad if you‚Äôre only talking about a small amount of data, transposing a few columns for a few rows is no big deal. But as data gets larger, or if you have to do this frequently, this process of transposing (AKA serialization) hurts."
  },
  {
    "objectID": "materials/slides/03-arrow.html#the-serialization-problem",
    "href": "materials/slides/03-arrow.html#the-serialization-problem",
    "title": "parquet, arrow, duckdb",
    "section": "The serialization problem",
    "text": "The serialization problem\n\n\nMany of these were operating in essentially column-oriented ways ‚Äî but to transfer data ended up writting into row-oriented data structures, then read them back in to something that was column-oriented.\nMoving data between representations is hard - Different formats, requirements, and limitations - Compatibility issues - Serialization is a huge bottleneck"
  },
  {
    "objectID": "materials/slides/03-arrow.html#the-serialization-problem-1",
    "href": "materials/slides/03-arrow.html#the-serialization-problem-1",
    "title": "parquet, arrow, duckdb",
    "section": "The serialization problem",
    "text": "The serialization problem"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-apache-arrow",
    "href": "materials/slides/03-arrow.html#what-is-apache-arrow",
    "title": "parquet, arrow, duckdb",
    "section": "What is Apache Arrow?",
    "text": "What is Apache Arrow?\n\n\nCross-language development platform for\nin-memory data\n\nConsistent in-memory columnar data format\nLanguage-independent\nZero-copy reads"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-apache-arrow-1",
    "href": "materials/slides/03-arrow.html#what-is-apache-arrow-1",
    "title": "parquet, arrow, duckdb",
    "section": "What is Apache Arrow?",
    "text": "What is Apache Arrow?\n\n\nBenefits:\n\nSeamless data interchange between systems\nFast analytical processing\nEfficient memory usage"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-apache-parquet",
    "href": "materials/slides/03-arrow.html#what-is-apache-parquet",
    "title": "parquet, arrow, duckdb",
    "section": "What is Apache Parquet?",
    "text": "What is Apache Parquet?\n\n\nOpen-source columnar storage format\n\nCreated by Twitter and Cloudera in 2013\nPart of the Apache Software Foundation"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-apache-parquet-1",
    "href": "materials/slides/03-arrow.html#what-is-apache-parquet-1",
    "title": "parquet, arrow, duckdb",
    "section": "What is Apache Parquet?",
    "text": "What is Apache Parquet?\n\n\nFeatures:\n\nColumnar storage\nExplicit schema\nStatistical metadata\nEfficient compression"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-a-file",
    "href": "materials/slides/03-arrow.html#reading-a-file",
    "title": "parquet, arrow, duckdb",
    "section": "Reading a File",
    "text": "Reading a File\nAs a CSV file\n\nsystem.time({\n  df &lt;- read.csv(\"CA_person_2021.csv\")\n})\n\n\n   user  system elapsed \n 14.449   0.445  15.037 \n\n\nDescribe the CSV\nThis CSV is 708 MB, I‚Äôm reading this in on my MacBook Pro, your times will vary! We can use arrow or data.tables‚Äôs CSV reader and it‚Äôs faster (1.85 sec and 1.61 sec respectively). And if we read to an arrow table it‚Äôs even faster: 0.51 seconds"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-a-file-1",
    "href": "materials/slides/03-arrow.html#reading-a-file-1",
    "title": "parquet, arrow, duckdb",
    "section": "Reading a File",
    "text": "Reading a File\nAs a Parquet file\n\nlibrary(arrow)\noptions(arrow.use_altrep = FALSE)\n\nsystem.time({\n  df &lt;- read_parquet(\"CA_person_2021.parquet\")\n})\n\n\n   user  system elapsed \n  1.017   0.207   0.568 \n\n\nThe parquet file is 62 MB\nIt‚Äôs even faster with altrep (0.186 s), but that‚Äôs cheating! Also, if we read into an arrow table rather than a dataframe: 0.1 second"
  },
  {
    "objectID": "materials/slides/03-arrow.html#exercise",
    "href": "materials/slides/03-arrow.html#exercise",
    "title": "parquet, arrow, duckdb",
    "section": "Exercise",
    "text": "Exercise\n\ndata &lt;- tibble::tibble(\n  integers = 1:10,\n  doubles = as.numeric(1:10),\n  strings = sprintf(\"%02d\", 1:10)\n)\n\nwrite.csv(data, \"numeric_base.csv\", row.names = FALSE)\nwrite_csv_arrow(data, \"numeric_arrow.csv\")\nwrite_parquet(data, \"numeric.parquet\")\n\ndf_csv &lt;- read.csv(\"numeric_base.csv\")\ndf_csv_arrow &lt;- read_csv_arrow(\"numeric_arrow.csv\")\ndf_parquet &lt;- read_parquet(\"numeric.parquet\")\n\n\nAre there any differences?"
  },
  {
    "objectID": "materials/slides/03-arrow.html#exercise-answer",
    "href": "materials/slides/03-arrow.html#exercise-answer",
    "title": "parquet, arrow, duckdb",
    "section": "Exercise (answer)",
    "text": "Exercise (answer)\n\n\n&gt; df_csv_arrow\n# A tibble: 10 √ó 3\n   integers doubles strings\n      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1        1       1       1\n 2        2       2       2\n 3        3       3       3\n 4        4       4       4\n 5        5       5       5\n 6        6       6       6\n 7        7       7       7\n 8        8       8       8\n 9        9       9       9\n10       10      10      10\n\n&gt; df_parquet\n# A tibble: 10 √ó 3\n   integers doubles strings\n      &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1       1 01     \n 2        2       2 02     \n 3        3       3 03     \n 4        4       4 04     \n 5        5       5 05     \n 6        6       6 06     \n 7        7       7 07     \n 8        8       8 08     \n 9        9       9 09     \n10       10      10 10"
  },
  {
    "objectID": "materials/slides/03-arrow.html#exercise-answer-1",
    "href": "materials/slides/03-arrow.html#exercise-answer-1",
    "title": "parquet, arrow, duckdb",
    "section": "Exercise (answer)",
    "text": "Exercise (answer)\n\n\n&gt; df_csv_arrow\n# A tibble: 10 √ó 3\n   integers doubles strings\n      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1        1       1       1\n 2        2       2       2\n 3        3       3       3\n 4        4       4       4\n 5        5       5       5\n 6        6       6       6\n 7        7       7       7\n 8        8       8       8\n 9        9       9       9\n10       10      10      10\n\n&gt; df_parquet\n# A tibble: 10 √ó 3\n   integers doubles strings\n      &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1       1 01     \n 2        2       2 02     \n 3        3       3 03     \n 4        4       4 04     \n 5        5       5 05     \n 6        6       6 06     \n 7        7       7 07     \n 8        8       8 08     \n 9        9       9 09     \n10       10      10 10"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-inside-a-parquet-file",
    "href": "materials/slides/03-arrow.html#what-is-inside-a-parquet-file",
    "title": "parquet, arrow, duckdb",
    "section": "What is inside a Parquet file?",
    "text": "What is inside a Parquet file?\n\nSchema metadata\n\nSelf-describing format\nPreserves column types\nType-safe data interchange\n\nThe data itself\n\nEncodings\nAdvanced compression\n\n\n\nEncodings: - Dictionary ‚Äî Particularly effective for categorical data - Run-length encoding - Efficient storage of sequential repeated values\nAdvanced compression:\n\nColumn-specific compression algorithms\nBoth dictionary and value compression"
  },
  {
    "objectID": "materials/slides/03-arrow.html#structure-of-a-parquet-file",
    "href": "materials/slides/03-arrow.html#structure-of-a-parquet-file",
    "title": "parquet, arrow, duckdb",
    "section": "Structure of a Parquet File",
    "text": "Structure of a Parquet File\n\n\n\nRow groups: Horizontal partitions of data\nColumn chunks: Columnar data within a row group\nPages: Small units of column chunk data\nFooter: Contains file metadata and schema"
  },
  {
    "objectID": "materials/slides/03-arrow.html#benchmarks-parquet-vs-csv",
    "href": "materials/slides/03-arrow.html#benchmarks-parquet-vs-csv",
    "title": "parquet, arrow, duckdb",
    "section": "Benchmarks: Parquet vs CSV",
    "text": "Benchmarks: Parquet vs CSV"
  },
  {
    "objectID": "materials/slides/03-arrow.html#benchmarks-parquet-vs-csv-1",
    "href": "materials/slides/03-arrow.html#benchmarks-parquet-vs-csv-1",
    "title": "parquet, arrow, duckdb",
    "section": "Benchmarks: Parquet vs CSV",
    "text": "Benchmarks: Parquet vs CSV"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-efficiency-selecting-columns",
    "href": "materials/slides/03-arrow.html#reading-efficiency-selecting-columns",
    "title": "parquet, arrow, duckdb",
    "section": "Reading Efficiency: Selecting Columns",
    "text": "Reading Efficiency: Selecting Columns\n\nWith CSV:\n\nMust read entire file, even if you only need a few columns\nNo efficient way to skip columns during read\n\nWith Parquet:\n\nRead only needed columns from disk\nSignificant performance benefit for wide tables"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-efficiency-selecting-columns-1",
    "href": "materials/slides/03-arrow.html#reading-efficiency-selecting-columns-1",
    "title": "parquet, arrow, duckdb",
    "section": "Reading Efficiency: Selecting Columns",
    "text": "Reading Efficiency: Selecting Columns\n\n\nsystem.time({\n  df_subset &lt;- read_parquet(\n    \"CA_person_2021.parquet\", \n    col_select = c(\"PUMA\", \"COW\")\n  )\n})\n\n\n\n\n   user  system elapsed \n  0.027   0.003   0.031 \n\n   user  system elapsed \n  1.017   0.207   0.568"
  },
  {
    "objectID": "materials/slides/03-arrow.html#nanoparquet-vs.-arrow-reader",
    "href": "materials/slides/03-arrow.html#nanoparquet-vs.-arrow-reader",
    "title": "parquet, arrow, duckdb",
    "section": "nanoparquet vs.¬†arrow Reader",
    "text": "nanoparquet vs.¬†arrow Reader\n\nnanoparquet\n\nLightweight Parquet reader\nMinimal dependencies\nGood for embedding\n\narrow\n\nFull-featured reader\nSupport for datasets\nIntegration with Arrow ecosystem"
  },
  {
    "objectID": "materials/slides/03-arrow.html#nanoparquet-vs.-arrow-reader-1",
    "href": "materials/slides/03-arrow.html#nanoparquet-vs.-arrow-reader-1",
    "title": "parquet, arrow, duckdb",
    "section": "nanoparquet vs.¬†arrow Reader",
    "text": "nanoparquet vs.¬†arrow Reader\n\nlibrary(arrow)\noptions(arrow.use_altrep = FALSE)\n\nsystem.time({\n  df &lt;- read_parquet(\"CA_person_2021.parquet\")\n})\n\n   user  system elapsed \n  1.017   0.207   0.568 \n\n\n\n\n\nlibrary(nanoparquet)\n\nsystem.time({\n  df &lt;- read_parquet(\"CA_person_2021.parquet\")\n})\n\n   user  system elapsed \n  0.709   0.099   0.894"
  },
  {
    "objectID": "materials/slides/03-arrow.html#parquet-tooling-ecosystem",
    "href": "materials/slides/03-arrow.html#parquet-tooling-ecosystem",
    "title": "parquet, arrow, duckdb",
    "section": "Parquet Tooling Ecosystem",
    "text": "Parquet Tooling Ecosystem\nLanguages with native Parquet support:\n\nR (via arrow, nanoparquet)\nPython (via pyarrow, pandas)\nJava\nC++\nRust\nJavaScript\nGo"
  },
  {
    "objectID": "materials/slides/03-arrow.html#parquet-tooling-ecosystem-1",
    "href": "materials/slides/03-arrow.html#parquet-tooling-ecosystem-1",
    "title": "parquet, arrow, duckdb",
    "section": "Parquet Tooling Ecosystem",
    "text": "Parquet Tooling Ecosystem\nSystems with Parquet integration:\n\nDuckDB\nGoogle BigQuery\nSnowflake\nAmazon Athena\nApache Spark\nApache Hadoop"
  },
  {
    "objectID": "materials/slides/03-arrow.html#introduction-to-the-arrow-package",
    "href": "materials/slides/03-arrow.html#introduction-to-the-arrow-package",
    "title": "parquet, arrow, duckdb",
    "section": "Introduction to the arrow Package",
    "text": "Introduction to the arrow Package\n\n# Install and load the Arrow package\ninstall.packages(\"arrow\")\nlibrary(arrow)\n\n# Check Arrow version and capabilities\narrow_info()\n\n\nThe arrow package provides:\n\nNative R interface to Apache Arrow\nTools for working with large datasets\nIntegration with dplyr for data manipulation\nReading/writing various file formats"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-and-writing-parquet-files-revisited",
    "href": "materials/slides/03-arrow.html#reading-and-writing-parquet-files-revisited",
    "title": "parquet, arrow, duckdb",
    "section": "Reading and Writing Parquet files, revisited",
    "text": "Reading and Writing Parquet files, revisited\n\n# Read a Parquet file into R\ndata &lt;- read_parquet(\"CA_person_2021.parquet\")\n\n# Write an R data frame to Parquet\nwrite_parquet(data, \"CA_person_2021_new.parquet\")\n\n# Reading a subset of columns\ndf_subset &lt;- read_parquet(\n  \"CA_person_2021.parquet\", \n  col_select = c(\"PUMA\", \"COW\", \"AGEP\")\n)\n\n# Reading with a row filter (predicate pushdown)\ndf_filtered &lt;- open_dataset(\"CA_person_2021.parquet\") |&gt; \n  filter(AGEP &gt; 40) |&gt;\n  collect()"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-using-dplyr-with-arrow",
    "href": "materials/slides/03-arrow.html#demo-using-dplyr-with-arrow",
    "title": "parquet, arrow, duckdb",
    "section": "Demo: Using dplyr with arrow",
    "text": "Demo: Using dplyr with arrow\n\n# Create an Arrow Table\ntable &lt;- read_parquet(\"CA_person_2021.parquet\", as_data_frame = FALSE)\n\n# Use dplyr verbs with arrow tables\ntable |&gt;\n  filter(AGEP &gt;= 16) |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |&gt;\n  collect()\n\n\nWithout as_data_frame = FALSE: the dataframe is backed by altrep, actually. But generally functions like any other dataframe."
  },
  {
    "objectID": "materials/slides/03-arrow.html#introduction-to-duckdb",
    "href": "materials/slides/03-arrow.html#introduction-to-duckdb",
    "title": "parquet, arrow, duckdb",
    "section": "Introduction to DuckDB",
    "text": "Introduction to DuckDB\n\n\nAnalytical SQL database system\n\nEmbedded database (like SQLite)\nColumn oriented\nIn-process query execution\n\nFeatures:\n\nDirect Parquet querying\nParallel processing\nZero-copy integration with arrow\n\n\n\nThe zero-copy integration with arrow is because DuckDB uses basically the same format for it‚Äôs own internal representation."
  },
  {
    "objectID": "materials/slides/03-arrow.html#duckdb",
    "href": "materials/slides/03-arrow.html#duckdb",
    "title": "parquet, arrow, duckdb",
    "section": "DuckDB",
    "text": "DuckDB\n\nlibrary(duckdb)\n\ncon &lt;- dbConnect(duckdb())\n\n# Register a Parquet file as a virtual table\ndbExecute(con, \"CREATE VIEW pums AS SELECT * \n                FROM read_parquet('CA_person_2021.parquet')\")\n\n# Run our query\ndbGetQuery(con, \"\n  SELECT SUM(JWMNP * PWGTP)/SUM(PWGTP) as avg_commute_time,\n         COUNT(*) as count\n  FROM pums\n  WHERE AGEP &gt;= 16\n\")\n\ndbDisconnect(con, shutdown = TRUE)"
  },
  {
    "objectID": "materials/slides/03-arrow.html#duckplyr",
    "href": "materials/slides/03-arrow.html#duckplyr",
    "title": "parquet, arrow, duckdb",
    "section": "duckplyr",
    "text": "duckplyr\n\nlibrary(duckplyr)\n\n# Read data with Arrow\npums_data &lt;- read_file_duckdb(\n  \"CA_person_2021.parquet\", \n  \"read_parquet\"\n)\n\n# Use duckplyr to optimize dplyr operations\npums_data |&gt;\n  filter(AGEP &gt;= 16) |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |&gt;\n  collect()\n\n\nduckplyr is a drop-in replacement for dplyr, using duckdb as a backend"
  },
  {
    "objectID": "materials/slides/03-arrow.html#data.table",
    "href": "materials/slides/03-arrow.html#data.table",
    "title": "parquet, arrow, duckdb",
    "section": "data.table",
    "text": "data.table\n\nlibrary(arrow)\nlibrary(data.table)\n\n# Read Parquet file with Arrow\npums_data &lt;- read_parquet(\"CA_person_2021.parquet\")\n\n# Convert to data.table\npums_dt &lt;- as.data.table(pums_data)\n\n# data.table query\npums_dt[AGEP &gt;= 16,\n  .(avg_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP), \n    count = .N)]"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-seamless-integration-arrow-duckdb",
    "href": "materials/slides/03-arrow.html#demo-seamless-integration-arrow-duckdb",
    "title": "parquet, arrow, duckdb",
    "section": "Demo: Seamless Integration Arrow ‚ÜîÔ∏é DuckDB",
    "text": "Demo: Seamless Integration Arrow ‚ÜîÔ∏é DuckDB\n\ntable &lt;- read_parquet(\"CA_person_2021.parquet\", as_data_frame = FALSE)\n\n# Use dplyr verbs with arrow tables\ntable |&gt;\n  filter(AGEP &gt;= 16) |&gt;\n  to_duckdb() |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  )"
  },
  {
    "objectID": "materials/slides/03-arrow.html#understanding-arrow-datasets-vs.-tables",
    "href": "materials/slides/03-arrow.html#understanding-arrow-datasets-vs.-tables",
    "title": "parquet, arrow, duckdb",
    "section": "Understanding Arrow Datasets vs.¬†Tables",
    "text": "Understanding Arrow Datasets vs.¬†Tables\n\n\nArrow Table\n\nIn-memory data structure\nMust fit in RAM\nFast operations\nSimilar to base data frames\nGood for single file data\n\n\nArrow Dataset\n\nCollection of files\nLazily evaluated\nLarger-than-memory capable\nDistributed execution\nSupports partitioning"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-querying-multi-file-datasets",
    "href": "materials/slides/03-arrow.html#demo-querying-multi-file-datasets",
    "title": "parquet, arrow, duckdb",
    "section": "Demo: Querying Multi-file Datasets",
    "text": "Demo: Querying Multi-file Datasets\n\npums_ds &lt;- open_dataset(\"data/person\")\n\n# Examine the dataset, list files\nprint(pums_ds)\nhead(pums_ds$files)\n\n# Query execution with lazy evaluation\npums_ds |&gt;\n  filter(AGEP &gt;= 16) |&gt;\n  group_by(year, ST) |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |&gt;\n  collect()\n\n\nMention Acero engine in C++"
  },
  {
    "objectID": "materials/slides/03-arrow.html#lazy-evaluation-and-query-optimization",
    "href": "materials/slides/03-arrow.html#lazy-evaluation-and-query-optimization",
    "title": "parquet, arrow, duckdb",
    "section": "Lazy Evaluation and Query Optimization",
    "text": "Lazy Evaluation and Query Optimization\n\nLazy evaluation workflow:\n\nDefine operations (filter, group, summarize)\nOptimizes the plan (predicate pushdown, et c.)\nOnly reads necessary data from disk\nExecutes when collect() is called\n\nBenefits:\n\nMinimizes memory usage + reduces I/O\nLeverages Arrow‚Äôs native compute functions"
  },
  {
    "objectID": "materials/slides/03-arrow.html#working-with-datasets-on-s3",
    "href": "materials/slides/03-arrow.html#working-with-datasets-on-s3",
    "title": "parquet, arrow, duckdb",
    "section": "Working with Datasets on S3",
    "text": "Working with Datasets on S3\narrow can work with data and datasets in cloud storage. This can be a good option if you don‚Äôt have access to a formal DBMS.\n\nEasy to store\narrow efficiently uses metadata to read only what is necessary\n\n\nI know, I know ‚Äî this workshop is about local files. But I couldn‚Äôt help myself"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-working-with-datasets-on-s3",
    "href": "materials/slides/03-arrow.html#demo-working-with-datasets-on-s3",
    "title": "parquet, arrow, duckdb",
    "section": "Demo: Working with Datasets on S3",
    "text": "Demo: Working with Datasets on S3\n\npums_ds &lt;- open_dataset(\"s3://scaling-arrow-pums/person/\")\n\n# Query execution with lazy evaluation\npums_ds |&gt;\n  filter(year == 2021, location == \"ca\", AGEP &gt;= 16) |&gt;\n  group_by(year, ST) |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |&gt;\n  collect()\n\n\nTalk about partitioning helping, etc"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-sipping-data",
    "href": "materials/slides/03-arrow.html#demo-sipping-data",
    "title": "parquet, arrow, duckdb",
    "section": "Demo: Sipping data",
    "text": "Demo: Sipping data\n\npums_ds &lt;- open_dataset(\"s3://scaling-arrow-pums/person/\")\n\n# Query execution with lazy evaluation\npums_ds |&gt;\n  filter(AGEP &gt;= 97) |&gt;\n  collect()\n\n\nAround ~110MB\nSys.getpid() nettop -p X"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-partitioning",
    "href": "materials/slides/03-arrow.html#what-is-partitioning",
    "title": "parquet, arrow, duckdb",
    "section": "What is Partitioning?",
    "text": "What is Partitioning?\n\nDividing data into logical segments\n\nStored in separate files/directories\nBased on one or more column values\nEnables efficient filtering\n\nBenefits:\n\nFaster queries that filter on partition columns\nImproved parallel processing\nEasier management of large datasets"
  },
  {
    "objectID": "materials/slides/03-arrow.html#hive-vs.-non-hive-partitioning",
    "href": "materials/slides/03-arrow.html#hive-vs.-non-hive-partitioning",
    "title": "parquet, arrow, duckdb",
    "section": "Hive vs.¬†Non-Hive Partitioning",
    "text": "Hive vs.¬†Non-Hive Partitioning\n\n\nHive Partitioning\n\nDirectory format: column=value\nExample:\nperson/\n‚îú‚îÄ‚îÄ year=2018/\n‚îÇ   ‚îú‚îÄ‚îÄ state=NY/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n‚îÇ   ‚îî‚îÄ‚îÄ state=CA/\n‚îÇ       ‚îî‚îÄ‚îÄ data.parquet\n‚îú‚îÄ‚îÄ year=2019/\n‚îÇ   ‚îú‚îÄ‚îÄ ...\nSelf-describing structure\nStandard in big data ecosystem\n\n\nNon-Hive Partitioning\n\nDirectory format: value\nExample:\nperson/\n‚îú‚îÄ‚îÄ 2018/\n‚îÇ   ‚îú‚îÄ‚îÄ NY/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n‚îÇ   ‚îî‚îÄ‚îÄ CA/\n‚îÇ       ‚îî‚îÄ‚îÄ data.parquet\n‚îú‚îÄ‚îÄ 2019/\n‚îÇ   ‚îú‚îÄ‚îÄ ...\nRequires column naming\nLess verbose directory names"
  },
  {
    "objectID": "materials/slides/03-arrow.html#effective-partitioning-strategies",
    "href": "materials/slides/03-arrow.html#effective-partitioning-strategies",
    "title": "parquet, arrow, duckdb",
    "section": "Effective Partitioning Strategies",
    "text": "Effective Partitioning Strategies\n\nChoose partition columns wisely:\n\nCommonly used in filters\nLow to medium cardinality\n\nCommon partition dimensions:\n\nTime (year, month, day)\nGeography (country, state, region)\nCategory (product type, department)"
  },
  {
    "objectID": "materials/slides/03-arrow.html#partitioning-in-practice-writing-datasets",
    "href": "materials/slides/03-arrow.html#partitioning-in-practice-writing-datasets",
    "title": "parquet, arrow, duckdb",
    "section": "Partitioning in Practice: Writing Datasets",
    "text": "Partitioning in Practice: Writing Datasets\n\nca_pums_data &lt;- read_parquet(\"CA_person_2021.parquet\")\n\nca_pums_data |&gt;\n  mutate(\n    age_group = case_when(\n      AGEP &lt; 18 ~ \"under_18\",\n      AGEP &lt; 30 ~ \"18_29\",\n      AGEP &lt; 45 ~ \"30_44\",\n      AGEP &lt; 65 ~ \"45_64\",\n      TRUE ~ \"65_plus\"\n    )\n  ) |&gt;\n  group_by(ST, age_group) |&gt;\n  write_dataset(\n    path = \"ca_pums_by_age/\"\n  )"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-repartitioning-the-whole-dataset",
    "href": "materials/slides/03-arrow.html#demo-repartitioning-the-whole-dataset",
    "title": "parquet, arrow, duckdb",
    "section": "Demo: Repartitioning the whole dataset",
    "text": "Demo: Repartitioning the whole dataset\n\npums_data &lt;- open_dataset(\"data/person\")\n\npums_data |&gt;\n  mutate(\n    age_group = case_when(\n      AGEP &lt; 18 ~ \"under_18\",\n      AGEP &lt; 30 ~ \"18_29\",\n      AGEP &lt; 45 ~ \"30_44\",\n      AGEP &lt; 65 ~ \"45_64\",\n      TRUE ~ \"65_plus\"\n    )\n  ) |&gt;\n  group_by(year, ST, age_group) |&gt;\n  write_dataset(\n    path = \"pums_by_age/\"\n  )\n\n\nuser system elapsed 386.530 65.862 115.496"
  },
  {
    "objectID": "materials/slides/03-arrow.html#best-practices-for-partition-design",
    "href": "materials/slides/03-arrow.html#best-practices-for-partition-design",
    "title": "parquet, arrow, duckdb",
    "section": "Best Practices for Partition Design",
    "text": "Best Practices for Partition Design\n\nAvoid over-partitioning:\n\nToo many small files = poor performance\nTarget file size: 20MB‚Äì2GB\nAvoid high-cardinality columns (e.g., user_id)\n\nConsider query patterns:\n\nPartition by commonly filtered columns\nBalance between read speed and write complexity"
  },
  {
    "objectID": "materials/slides/03-arrow.html#partitioning-performance-impact",
    "href": "materials/slides/03-arrow.html#partitioning-performance-impact",
    "title": "parquet, arrow, duckdb",
    "section": "Partitioning Performance Impact",
    "text": "Partitioning Performance Impact\n\n\nopen_dataset(\"&lt;path/to/data&gt;\") |&gt;\n  filter(year &gt;= 2018) |&gt;\n  summarise(\n    mean_commute = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP)\n  ) |&gt;\n  collect()"
  },
  {
    "objectID": "materials/slides/03-arrow.html#conclusion",
    "href": "materials/slides/03-arrow.html#conclusion",
    "title": "parquet, arrow, duckdb",
    "section": "Conclusion",
    "text": "Conclusion\n\nColumn-oriented storage formats like Parquet provide massive performance advantages for analytical workloads (30x speed, 10x smaller files)\nPartitioning strategies help manage large datasets effectively when working with data too big for memory"
  },
  {
    "objectID": "materials/slides/03-arrow.html#conclusion-1",
    "href": "materials/slides/03-arrow.html#conclusion-1",
    "title": "parquet, arrow, duckdb",
    "section": "Conclusion",
    "text": "Conclusion\n\nApache Arrow enables seamless data interchange between systems without costly serialization/deserialization\nMultiple query engines (arrow, DuckDB, data.table) offer flexibility depending on your analysis needs, all using modern formats like Parquet"
  },
  {
    "objectID": "materials/slides/03-arrow.html#conclusion-2",
    "href": "materials/slides/03-arrow.html#conclusion-2",
    "title": "parquet, arrow, duckdb",
    "section": "Conclusion",
    "text": "Conclusion\nResources:\n\nWorkshop materials: GitHub Repository\nArrow documentation: arrow.apache.org/docs/r\nParquet: parquet.apache.org\nDuckDB: duckdb.org\nBook: Scaling up with Arrow and R\n\nQuestions?"
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Short Course Presenters",
    "section": "",
    "text": "Dr.¬†Kelly Bodwin is an educator with over a decade of experience of teaching statistics and data science with R. She has co-authored multiple R packages, including flair and tidyclust, and she is currently a consultant on an NSF Grant building infrastructure for the data.table package. Her published applied research frequently involves manipulating large, in-memory data. Examples include: performing large matrix computations on high-dimensional GWAS (genome-wide association studies) data; constructing temporal social networks at hundreds of time checkpoints for organizational membership data; and summarizing biodiversity metrics grouped over exhaustive permutations of taxa level organism counts and experimental conditions. Above all, Dr.¬†Bodwin‚Äôs educational goal is to lower barriers to entry for beginner and intermediate R users to benefit from modern tools and enable more efficient and effective data workflows.\n\n\n\nDr.¬†Tyson Barrett is a researcher and an applied statistician at Highmark Health and Utah State University. He has over 15 years of R package development and programming experience, including maintaining data.table (with over 600,000 monthly downloads) and 3 other published R packages. He is currently a consultant on an NSF Grant building infrastructure for the data.table package. In his research work, he regularly works with large datasets with millions of rows and hundreds of columns. He and his team use data.table, arrow, and duckDB daily to manage and analyze their data to efficiently and quickly communicate insights with stakeholders.\n\n\n\nDr.¬†Jonathan Keane is an engineering leader at Posit, PBC with a background in data science and social science. They have been building data tooling for 15 years, including both R and Python data tools for scientific and data science computing. They are a member of the PMC for Apache Arrow, a maintainer of the Apache Arrow package, and the author of dittodb. They have also worked as a data scientist in a number of different industries (identify verification and fraud, market research, call centers, and social justice among other areas) using a wide range of tools to analyze, model, and use data at large enterprise scales. On top of building data tooling, they have a passion for teaching data scientists and others how to use data and tools to do their work and inform their decisions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dev/Outline_draft.html",
    "href": "dev/Outline_draft.html",
    "title": "Course Outline",
    "section": "",
    "text": "How big is my data? Long, wide or both?\nHow big is my analysis? Big because many groups? Big because looping? Big because matrix computation?\nHow often will I repeat my analysis? By myself as I refine it? In production?\nHow to choose a tool? -&gt; You have one dataset. It can read into R, but tasks are slow:\ngrouping and summarizing, esp over a variable with many cats\nmapping through a column, e.g to string process or custom function\ncomputing lags, sliders, etc.\nmaking many new columns\n\nWhat‚Äôs my slowdown? -&gt; profvis\nPersona: Survey research\n-&gt; You have one dataset. It is too large to read into R.\nPersona: Biological genetic data\nWhat‚Äôs my slowdown? -&gt; read a few lines, develop your pipeline, profvis\n-&gt; You have many datasets. At least one is too large to read into R. Joining and subsetting is slow.\nPersona: Customer data\n\nWhat makes some code faster than others?\nEfficiency of pipeline: filter before mutate.\nEfficiency of algorithm: think matrix order\nEfficiency of memory handling: careful C allocations\nEfficiency of interpreting or compiling\nEfficiency of data storage structure\nSmart saving of intermediate objects.\n\nx: data size y: number of repeated operations\n1,1: any 1,2: data table. [write more efficient code] 2,1: parquet & arrow [use better storage formats] 2,2: duckdb & arrow [run in SQL not R]\nhigher x -&gt; cloud database higher y -&gt; bigger machine/parallelize across machines/whatever\n\n\n\n\n\n\nsmall: &lt; 1000 or something, everything is split second\nmedium: 100000 or so, and/or many categories, can read into R but analysis is slow\nlarge: parquet can store on local machine\nmassive: too big for local machine\nQuestions for Tyson/Jon: - Easy way to convert csv to parquet w/o reading into R?\n- Column and row size; handled differently in R? In parquet? In a db?\nTitle of course Storing, Importing, Managing, and Analyzing Large Data Locally with R Instructor 1 Kelly Bodwin Instructor 1 Email kbodwin@calpoly.edu Instructor 2 Tyson Barrett Instructor 2 Email tyson.barrett@usu.edu Instructor 3 Jonathan Keane Instructor 3 Email jkeane@gmail.com Length of Course\nFull-day (7.5 contact hours) Course Description It is increasingly common in academic and professional settings to encounter datasets large enough to exceed the capabilities of standard data processing tools, yet small enough to be stored on local computers. Recent articles even claim that ‚Äúthe era of big data is over‚Äù and that data analysts and researchers should ‚Äúthink small, develop locally, ship joyfully.‚Äù Such ‚Äúmedium‚Äù dataests are instrumental in measuring, tracking, and recording a wide array of phenomena across disciplines such as human behavior, animal studies, geology, economics, and astronomy. In this workshop, we will present modern techniques for handling large local data in R using a tidy data pipeline, encompassing stages from data storage and importing to cleaning, analysis, and exporting data and analyses. Specifically, we will teach a combination of tools from the data.table, arrow, and duckDB packages, with a focus on parquet data files for storage and transfer. By the end of the workshop, participants will understand how to integrate these tools to establish a legible, reproducible, efficient, and high-performance workflow.\nCourse Outline The following outline shows our planned approach to managing and analyzing large data locally in R. Our target audience are individuals in academic or professional data analysis positions, who work regularly with datasets that are manageable in terms of local storage but pose significant challenges in processing and cleaning due to their size and complexity.\nUnit 1: Identifying slowdowns in your local data process (Bodwin; 1 hour)\n1.1 Finding the problem:\n- User-friendly code timing with tictoc - Comparing runtimes with atime - Code profiling with profvis\n1.2 Categories of bottlenecks - Common scenarios for repeated runs of code sections - Speed impact from order-of-operations in data wrangling - Fast vs.¬†slow types of dataset operations in R\n1.3 Activity: Code-along demo - Walkthrough of common data structures and tasks that could benefit from modern large-data tools\nUnit 2: In-Memory data wrangling with data.table (Barrett; 2 hours)\n2.1 Introduction - Basic syntax and structure of data.table - Speed comparison for common simple data tasks - High-level, user-friendly intuition for data.table‚Äôs ‚Äúunder the hood‚Äù parallel processing and C optimization\n2.2 Data wrangling tools - Filtering, summarizing, grouping, and mutating data - Sophisticated data processing with the set* functions.\n2.3 Activity: Code-Along - Real-data examples of data.table use for processing and analyzing data.\n2.4 Reference semantics - Speed and memory gains from modify-by-reference - Effects and side-effects of modify-by-reference - data.table syntax for fast no-copy data transformation\n2.5 Helper packages - Brief highlight of dtplyr and tidyfast as syntactical wrappers to data.table - Storing and reading data.table objects with parquet.\n2.6 Activity: Case Study - Learners work through a guided but incomplete real-data analysis.\nUnit 3: Storing, Reading, and Converting data with arrow, parquet, and duckdb (Keane 2 hours, Bodwin/Barrett 1 hour)\n3.1 Introduction to Arrow and Parquet - Intro to history and development of Arrow - Basic Arrow infrastructure and syntax - Discussion of the interchange problem - Using arrow reader and nanoparquet for efficient dataset storage and input. - Discussion of the Parquet structure, including column orientation and its benefits\n3.2 Activity: Code-Along - Data analysis with Arrow.\n3.3 Introduction to DuckDB - Introduction to duckDB and the local database model. - Basic duckDB syntax. - Data processing and analysis in duckDB - Helper packages such as duckplyr. - Working with duckDB and parquet files simultaneously.\n3.4 Activity: Code-Along - Data analysis with duckDB.\n3.5 Comparison of tools - Similarities and trade-offs of arrow, duckDB, and data.table. - Options for dplyr syntax in all three packages.\n3.6 Activity: Case Study - Goals: Compare, contrast, and benchmark - Learners repeat a data analysis task three times, using each of the three tools. - Learners benchmark the speed of each step of the task in the three implementations. - Discussion and reflection on learner-preferred syntax and usage.\nUnit 4: Putting it together: a workflow for efficient data manipulation (Bodwin/Barrett 1.5 hours)\n4.1 Showcase: A tidy pipeline using these modern, efficient tools - Import/export: fread, parquet with arrow/duckDB - Tidy: dtplyr, duckplyr, arrow - Transform: dtplyr, duckplyr, arrow\n4.2 Decisions and Guidelines - When to choose fread with csv versus parquet conversion. - Pros and cons of the local database structure versus local raw data files. - Specific data sizes, formats, and computations that are best suited to each tool.\n4.3 Activity: Final Case Study - Learners take ownership of a case study of real world large data, writing their own code with a large dataset from start to finish with instructor support Learning Objectives Diagnosing and Benchmarking: - Incorporate time checks into a data analysis workflow to identify slowdowns. - Recognize workflow sections that are likely to be re-run - Design data pipeline steps to isolate and improve bottlenecks\nSyntax: - Write basic data analysis code in data.table - Write basic data analysis code in arrow - Write basic data analysis code in duckDB - Write dplyr syntax code with dtplyr and duckplyr\nConcepts and Ideas: - Recognize grouping and summarizing operations that will benefit from the data.table implementation - Understand the modify-by-reference approach - Understand the benefits of parquet‚Äôs column orientation storage - Know the difference between a collection of local files and a local database.\nWorkflow: - Read csv data with fread and parquet format data with arrow - Set up and read/write data in a local duckDB database - Smoothly switch between major large data tools within a single data processing and analyzing pipeline. Instructor(s) Background Dr.¬†Kelly Bodwin is an educator with over a decade of experience of teaching statistics and data science with R. She has co-authored multiple R packages, including flair and tidyclust, and she is currently a consultant on an NSF Grant building infrastructure for the data.table package. Her published applied research frequently involves manipulating large, in-memory data. Examples include: performing large matrix computations on high-dimensional GWAS (genome-wide association studies) data; constructing temporal social networks at hundreds of time checkpoints for organizational membership data; and summarizing biodiversity metrics grouped over exhaustive permutations of taxa level organism counts and experimental conditions. Above all, Dr.¬†Bodwin‚Äôs educational goal is to lower barriers to entry for beginner and intermediate R users to benefit from modern tools and enable more efficient and effective data workflows.\nDr.¬†Tyson Barrett is a researcher and an applied statistician at Highmark Health and Utah State University. He has over 15 years of R package development and programming experience, including maintaining data.table (with over 600,000 monthly downloads) and 3 other published R packages. He is currently a consultant on an NSF Grant building infrastructure for the data.table package. In his research work, he regularly works with large datasets with millions of rows and hundreds of columns. He and his team use data.table, arrow, and duckDB daily to manage and analyze their data to efficiently and quickly communicate insights with stakeholders.\nDr.¬†Jonathan Keane is an engineering leader at Posit, PBC with a background in data science and social science. They have been building data tooling for 15 years, including both R and Python data tools for scientific and data science computing. They are a member of the PMC for Apache Arrow, a maintainer of the Apache Arrow package, and the author of dittodb. They have also worked as a data scientist in a number of different industries (identify verification and fraud, market research, call centers, and social justice among other areas) using a wide range of tools to analyze, model, and use data at large enterprise scales. On top of building data tooling, they have a passion for teaching data scientists and others how to use data and tools to do their work and inform their decisions.\nAdditional Comments Learners should bring a working laptop with an installation of R 4.0+ and a recent (2023 or later) installation of RStudio or Positron. Learners should ensure that their laptop has admin permission for installation of new R packages.\nA beginner-intermediate level of working knowledge in R with the tidyverse is assumed; at approximately the level of Chapters 1-8 in Wickham‚Äôs R for Data Science (2e)."
  },
  {
    "objectID": "dev/Outline_draft.html#unit-one-defining-the-problem",
    "href": "dev/Outline_draft.html#unit-one-defining-the-problem",
    "title": "Course Outline",
    "section": "",
    "text": "How big is my data? Long, wide or both?\nHow big is my analysis? Big because many groups? Big because looping? Big because matrix computation?\nHow often will I repeat my analysis? By myself as I refine it? In production?\nHow to choose a tool? -&gt; You have one dataset. It can read into R, but tasks are slow:\ngrouping and summarizing, esp over a variable with many cats\nmapping through a column, e.g to string process or custom function\ncomputing lags, sliders, etc.\nmaking many new columns\n\nWhat‚Äôs my slowdown? -&gt; profvis\nPersona: Survey research\n-&gt; You have one dataset. It is too large to read into R.\nPersona: Biological genetic data\nWhat‚Äôs my slowdown? -&gt; read a few lines, develop your pipeline, profvis\n-&gt; You have many datasets. At least one is too large to read into R. Joining and subsetting is slow.\nPersona: Customer data\n\nWhat makes some code faster than others?\nEfficiency of pipeline: filter before mutate.\nEfficiency of algorithm: think matrix order\nEfficiency of memory handling: careful C allocations\nEfficiency of interpreting or compiling\nEfficiency of data storage structure\nSmart saving of intermediate objects.\n\nx: data size y: number of repeated operations\n1,1: any 1,2: data table. [write more efficient code] 2,1: parquet & arrow [use better storage formats] 2,2: duckdb & arrow [run in SQL not R]\nhigher x -&gt; cloud database higher y -&gt; bigger machine/parallelize across machines/whatever"
  },
  {
    "objectID": "dev/Outline_draft.html#unit-three-handle-it-in-a-local-database-with-arrow-and-duckdb",
    "href": "dev/Outline_draft.html#unit-three-handle-it-in-a-local-database-with-arrow-and-duckdb",
    "title": "Course Outline",
    "section": "",
    "text": "small: &lt; 1000 or something, everything is split second\nmedium: 100000 or so, and/or many categories, can read into R but analysis is slow\nlarge: parquet can store on local machine\nmassive: too big for local machine\nQuestions for Tyson/Jon: - Easy way to convert csv to parquet w/o reading into R?\n- Column and row size; handled differently in R? In parquet? In a db?\nTitle of course Storing, Importing, Managing, and Analyzing Large Data Locally with R Instructor 1 Kelly Bodwin Instructor 1 Email kbodwin@calpoly.edu Instructor 2 Tyson Barrett Instructor 2 Email tyson.barrett@usu.edu Instructor 3 Jonathan Keane Instructor 3 Email jkeane@gmail.com Length of Course\nFull-day (7.5 contact hours) Course Description It is increasingly common in academic and professional settings to encounter datasets large enough to exceed the capabilities of standard data processing tools, yet small enough to be stored on local computers. Recent articles even claim that ‚Äúthe era of big data is over‚Äù and that data analysts and researchers should ‚Äúthink small, develop locally, ship joyfully.‚Äù Such ‚Äúmedium‚Äù dataests are instrumental in measuring, tracking, and recording a wide array of phenomena across disciplines such as human behavior, animal studies, geology, economics, and astronomy. In this workshop, we will present modern techniques for handling large local data in R using a tidy data pipeline, encompassing stages from data storage and importing to cleaning, analysis, and exporting data and analyses. Specifically, we will teach a combination of tools from the data.table, arrow, and duckDB packages, with a focus on parquet data files for storage and transfer. By the end of the workshop, participants will understand how to integrate these tools to establish a legible, reproducible, efficient, and high-performance workflow.\nCourse Outline The following outline shows our planned approach to managing and analyzing large data locally in R. Our target audience are individuals in academic or professional data analysis positions, who work regularly with datasets that are manageable in terms of local storage but pose significant challenges in processing and cleaning due to their size and complexity.\nUnit 1: Identifying slowdowns in your local data process (Bodwin; 1 hour)\n1.1 Finding the problem:\n- User-friendly code timing with tictoc - Comparing runtimes with atime - Code profiling with profvis\n1.2 Categories of bottlenecks - Common scenarios for repeated runs of code sections - Speed impact from order-of-operations in data wrangling - Fast vs.¬†slow types of dataset operations in R\n1.3 Activity: Code-along demo - Walkthrough of common data structures and tasks that could benefit from modern large-data tools\nUnit 2: In-Memory data wrangling with data.table (Barrett; 2 hours)\n2.1 Introduction - Basic syntax and structure of data.table - Speed comparison for common simple data tasks - High-level, user-friendly intuition for data.table‚Äôs ‚Äúunder the hood‚Äù parallel processing and C optimization\n2.2 Data wrangling tools - Filtering, summarizing, grouping, and mutating data - Sophisticated data processing with the set* functions.\n2.3 Activity: Code-Along - Real-data examples of data.table use for processing and analyzing data.\n2.4 Reference semantics - Speed and memory gains from modify-by-reference - Effects and side-effects of modify-by-reference - data.table syntax for fast no-copy data transformation\n2.5 Helper packages - Brief highlight of dtplyr and tidyfast as syntactical wrappers to data.table - Storing and reading data.table objects with parquet.\n2.6 Activity: Case Study - Learners work through a guided but incomplete real-data analysis.\nUnit 3: Storing, Reading, and Converting data with arrow, parquet, and duckdb (Keane 2 hours, Bodwin/Barrett 1 hour)\n3.1 Introduction to Arrow and Parquet - Intro to history and development of Arrow - Basic Arrow infrastructure and syntax - Discussion of the interchange problem - Using arrow reader and nanoparquet for efficient dataset storage and input. - Discussion of the Parquet structure, including column orientation and its benefits\n3.2 Activity: Code-Along - Data analysis with Arrow.\n3.3 Introduction to DuckDB - Introduction to duckDB and the local database model. - Basic duckDB syntax. - Data processing and analysis in duckDB - Helper packages such as duckplyr. - Working with duckDB and parquet files simultaneously.\n3.4 Activity: Code-Along - Data analysis with duckDB.\n3.5 Comparison of tools - Similarities and trade-offs of arrow, duckDB, and data.table. - Options for dplyr syntax in all three packages.\n3.6 Activity: Case Study - Goals: Compare, contrast, and benchmark - Learners repeat a data analysis task three times, using each of the three tools. - Learners benchmark the speed of each step of the task in the three implementations. - Discussion and reflection on learner-preferred syntax and usage.\nUnit 4: Putting it together: a workflow for efficient data manipulation (Bodwin/Barrett 1.5 hours)\n4.1 Showcase: A tidy pipeline using these modern, efficient tools - Import/export: fread, parquet with arrow/duckDB - Tidy: dtplyr, duckplyr, arrow - Transform: dtplyr, duckplyr, arrow\n4.2 Decisions and Guidelines - When to choose fread with csv versus parquet conversion. - Pros and cons of the local database structure versus local raw data files. - Specific data sizes, formats, and computations that are best suited to each tool.\n4.3 Activity: Final Case Study - Learners take ownership of a case study of real world large data, writing their own code with a large dataset from start to finish with instructor support Learning Objectives Diagnosing and Benchmarking: - Incorporate time checks into a data analysis workflow to identify slowdowns. - Recognize workflow sections that are likely to be re-run - Design data pipeline steps to isolate and improve bottlenecks\nSyntax: - Write basic data analysis code in data.table - Write basic data analysis code in arrow - Write basic data analysis code in duckDB - Write dplyr syntax code with dtplyr and duckplyr\nConcepts and Ideas: - Recognize grouping and summarizing operations that will benefit from the data.table implementation - Understand the modify-by-reference approach - Understand the benefits of parquet‚Äôs column orientation storage - Know the difference between a collection of local files and a local database.\nWorkflow: - Read csv data with fread and parquet format data with arrow - Set up and read/write data in a local duckDB database - Smoothly switch between major large data tools within a single data processing and analyzing pipeline. Instructor(s) Background Dr.¬†Kelly Bodwin is an educator with over a decade of experience of teaching statistics and data science with R. She has co-authored multiple R packages, including flair and tidyclust, and she is currently a consultant on an NSF Grant building infrastructure for the data.table package. Her published applied research frequently involves manipulating large, in-memory data. Examples include: performing large matrix computations on high-dimensional GWAS (genome-wide association studies) data; constructing temporal social networks at hundreds of time checkpoints for organizational membership data; and summarizing biodiversity metrics grouped over exhaustive permutations of taxa level organism counts and experimental conditions. Above all, Dr.¬†Bodwin‚Äôs educational goal is to lower barriers to entry for beginner and intermediate R users to benefit from modern tools and enable more efficient and effective data workflows.\nDr.¬†Tyson Barrett is a researcher and an applied statistician at Highmark Health and Utah State University. He has over 15 years of R package development and programming experience, including maintaining data.table (with over 600,000 monthly downloads) and 3 other published R packages. He is currently a consultant on an NSF Grant building infrastructure for the data.table package. In his research work, he regularly works with large datasets with millions of rows and hundreds of columns. He and his team use data.table, arrow, and duckDB daily to manage and analyze their data to efficiently and quickly communicate insights with stakeholders.\nDr.¬†Jonathan Keane is an engineering leader at Posit, PBC with a background in data science and social science. They have been building data tooling for 15 years, including both R and Python data tools for scientific and data science computing. They are a member of the PMC for Apache Arrow, a maintainer of the Apache Arrow package, and the author of dittodb. They have also worked as a data scientist in a number of different industries (identify verification and fraud, market research, call centers, and social justice among other areas) using a wide range of tools to analyze, model, and use data at large enterprise scales. On top of building data tooling, they have a passion for teaching data scientists and others how to use data and tools to do their work and inform their decisions.\nAdditional Comments Learners should bring a working laptop with an installation of R 4.0+ and a recent (2023 or later) installation of RStudio or Positron. Learners should ensure that their laptop has admin permission for installation of new R packages.\nA beginner-intermediate level of working knowledge in R with the tidyverse is assumed; at approximately the level of Chapters 1-8 in Wickham‚Äôs R for Data Science (2e)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JSM 2025 Short Course",
    "section": "",
    "text": "by: Kelly Bodwin, Tyson Barrett & Jonathan Keane"
  },
  {
    "objectID": "index.html#large-local-data",
    "href": "index.html#large-local-data",
    "title": "JSM 2025 Short Course",
    "section": "",
    "text": "by: Kelly Bodwin, Tyson Barrett & Jonathan Keane"
  },
  {
    "objectID": "index.html#short-course-description",
    "href": "index.html#short-course-description",
    "title": "JSM 2025 Short Course",
    "section": "Short Course Description",
    "text": "Short Course Description\nIt is increasingly common in academic and professional settings to encounter datasets large enough to exceed the capabilities of standard data processing tools, yet small enough to be stored on local computers. Recent articles even claim that ‚Äúthe era of big data is over‚Äù and that data analysts and researchers should ‚Äúthink small, develop locally, ship joyfully‚Äù Such ‚Äúmedium‚Äù dataests are instrumental in measuring, tracking, and recording a wide array of phenomena across disciplines such as human behavior, animal studies, geology, economics, and astronomy. In this workshop, we will present modern techniques for handling large local data in R using a tidy data pipeline, encompassing stages from data storage and importing to cleaning, analysis, and exporting data and analyses. Specifically, we will teach a combination of tools from the data.table, arrow, and duckDB packages, with a focus on parquet data files for storage and transfer. By the end of the workshop, participants will understand how to integrate these tools to establish a legible, reproducible, efficient, and high-performance workflow."
  },
  {
    "objectID": "index.html#intended-audience-and-level",
    "href": "index.html#intended-audience-and-level",
    "title": "JSM 2025 Short Course",
    "section": "Intended Audience and Level",
    "text": "Intended Audience and Level\nWe expect attendees to have R fluency at the level of a typical introductory course, such as the textbook R for Data Science (Wickham, √áetinkaya-Rundel, & Grolemund 2023); as well as familiarity with some data application that may motivate tools beyond the introductory level."
  },
  {
    "objectID": "materials/activities/03-practice-arrow.html",
    "href": "materials/activities/03-practice-arrow.html",
    "title": "Hands-on practice: Analysis with PUMS Data",
    "section": "",
    "text": "ProblemsHintsSolution\n\n\n\nLoad the PUMS person data as an arrow dataset\nWhat are the dimensions of the dataset? Number of rows? Number of columns?\nExamine the dataset:\n\nWhat columns does the dataset have?\nWhat are the column types?\n\n\n\n\nYou can use arrow::open_dataset(...) to open a dataset. Treat the dataset like it‚Äôs a data frame, can you use the same functions that you are already familiar with?\n\n\n\nlibrary(arrow)\nlibrary(dplyr)\n\n# Open the PUMS dataset\npums_ds &lt;- open_dataset(\"~/PUMS/data/person\")\n\n# dimensions\nnrow(pums_ds)\nncol(pums_ds)\n\n# Examine the dataset\npums_ds\n\n# Examine schema\nschema(pums_ds)"
  },
  {
    "objectID": "materials/activities/03-practice-arrow.html#exercise-1-reading-partitioned-pums-data",
    "href": "materials/activities/03-practice-arrow.html#exercise-1-reading-partitioned-pums-data",
    "title": "Hands-on practice: Analysis with PUMS Data",
    "section": "",
    "text": "ProblemsHintsSolution\n\n\n\nLoad the PUMS person data as an arrow dataset\nWhat are the dimensions of the dataset? Number of rows? Number of columns?\nExamine the dataset:\n\nWhat columns does the dataset have?\nWhat are the column types?\n\n\n\n\nYou can use arrow::open_dataset(...) to open a dataset. Treat the dataset like it‚Äôs a data frame, can you use the same functions that you are already familiar with?\n\n\n\nlibrary(arrow)\nlibrary(dplyr)\n\n# Open the PUMS dataset\npums_ds &lt;- open_dataset(\"~/PUMS/data/person\")\n\n# dimensions\nnrow(pums_ds)\nncol(pums_ds)\n\n# Examine the dataset\npums_ds\n\n# Examine schema\nschema(pums_ds)"
  },
  {
    "objectID": "materials/activities/03-practice-arrow.html#exercise-2-basic-filtering-and-aggregation",
    "href": "materials/activities/03-practice-arrow.html#exercise-2-basic-filtering-and-aggregation",
    "title": "Hands-on practice: Analysis with PUMS Data",
    "section": "Exercise 2: Basic Filtering and Aggregation",
    "text": "Exercise 2: Basic Filtering and Aggregation\n\nProblemsHintSolution\n\n\n\nWhat was the population of Tennessee in 2018?\nWhat was the population of Tennessee over the last five years of available data?\n\n\n\nTry using familiar dplyr verbs: filter(), summarize(), et c.\n\n\n\nlibrary(arrow)\nlibrary(dplyr)\n\n# Open the PUMS dataset\npums_ds &lt;- open_dataset(\"~/PUMS/data/person\")\n\npums_ds |&gt;\n  filter(year == 2018, location == \"tn\") |&gt;\n  summarize(\n    n = n(),\n    population = sum(PWGTP)\n  ) |&gt;\n  collect()\n\npums_ds |&gt;\n  filter(location == \"tn\") |&gt;\n  group_by(year) |&gt;\n  summarize(\n    n = n(),\n    population = sum(PWGTP)\n  ) |&gt;\n  arrange(-year) |&gt;\n  slice_head(n = 5) |&gt;\n  collect()"
  },
  {
    "objectID": "materials/activities/03-practice-arrow.html#exercise-3-using-duckdb-with-pums-data",
    "href": "materials/activities/03-practice-arrow.html#exercise-3-using-duckdb-with-pums-data",
    "title": "Hands-on practice: Analysis with PUMS Data",
    "section": "Exercise 3: Using DuckDB with PUMS Data",
    "text": "Exercise 3: Using DuckDB with PUMS Data\n\nProblemsHintSolution\n\n\nFor each state, compute the minimum commute time and the maximum commute time. Return it in a long data frame (hint, you‚Äôll need to use tidyr::pivot_longer here) with the columns: location, commute_metric (values: max_commute, min_commute), time.\nFor motivation: if you were trying to plot using ggplot2 and wanted both the minimums and the maximums to show up on the same plot.\n\n\nArrow‚Äôs query engine doesn‚Äôt support tidyr::pivot_longer. So you‚Äôll need to use to_duckdb() and then use duckdb from there.\n\n\n\nlibrary(arrow)\nlibrary(duckdb)\nlibrary(tidyr)\n\n# Open the PUMS dataset\npums_ds &lt;- open_dataset(\"~/PUMS/data/person\")\n\npums_ds |&gt;\n  group_by(location) |&gt;\n  summarize(\n    max_commute = max(JWMNP, na.rm = TRUE),\n    min_commute = min(JWMNP, na.rm = TRUE)\n  ) |&gt;\n  to_duckdb() |&gt; \n  pivot_longer(!location, names_to = \"commute_metric\", values_to = \"time\") |&gt;\n  arrange(location) |&gt;\n  collect()"
  },
  {
    "objectID": "materials/activities/03-practice-arrow.html#challenge-formulate-your-own-analysis",
    "href": "materials/activities/03-practice-arrow.html#challenge-formulate-your-own-analysis",
    "title": "Hands-on practice: Analysis with PUMS Data",
    "section": "Challenge: Formulate Your Own Analysis",
    "text": "Challenge: Formulate Your Own Analysis\n\nChoose a research question:\n\nHow has commute time changed over the years?\nWhat‚Äôs the relationship between education and income?\nHow does housing cost burden vary by state?\nYour own question‚Ä¶\n\nImplement the analysis using:\n\nArrow Dataset operations\nDuckDB SQL queries\nData visualization\n\nCompare performance between approaches"
  },
  {
    "objectID": "materials/activities/03-practice-arrow.html#the-pums-dataset",
    "href": "materials/activities/03-practice-arrow.html#the-pums-dataset",
    "title": "Hands-on practice: Analysis with PUMS Data",
    "section": "The PUMS Dataset",
    "text": "The PUMS Dataset\ndetailed dataset description\n\nPublic Use Microdata Sample\n\nUS Census Bureau data\nIndividual person and household records\nAnonymized demographic information\nIncome, education, housing, commute, et c.\n\nDataset characteristics:\n\nMultiple years (2005-2022, sans 2020)\nAll US states and territories\n53 Million rows (person), 25 Million rows (household)\n200+ variables"
  },
  {
    "objectID": "materials/activities/04-practice-workflow.html#demo",
    "href": "materials/activities/04-practice-workflow.html#demo",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "materials/activities/04-practice-workflow.html#your-turn",
    "href": "materials/activities/04-practice-workflow.html#your-turn",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Your Turn",
    "text": "Your Turn"
  }
]