[
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Short Course Presenters",
    "section": "",
    "text": "Dr. Kelly Bodwin is an educator with over a decade of experience of teaching statistics and data science with R. She has co-authored multiple R packages, including flair and tidyclust, and she is currently a consultant on an NSF Grant building infrastructure for the data.table package. Her published applied research frequently involves manipulating large, in-memory data. Examples include: performing large matrix computations on high-dimensional GWAS (genome-wide association studies) data; constructing temporal social networks at hundreds of time checkpoints for organizational membership data; and summarizing biodiversity metrics grouped over exhaustive permutations of taxa level organism counts and experimental conditions. Above all, Dr. Bodwin’s educational goal is to lower barriers to entry for beginner and intermediate R users to benefit from modern tools and enable more efficient and effective data workflows.\n\n\n\nDr. Tyson Barrett is a researcher and an applied statistician at Highmark Health and Utah State University. He has over 15 years of R package development and programming experience, including maintaining data.table (with over 600,000 monthly downloads) and 3 other published R packages. He is currently a consultant on an NSF Grant building infrastructure for the data.table package. In his research work, he regularly works with large datasets with millions of rows and hundreds of columns. He and his team use data.table, arrow, and duckDB daily to manage and analyze their data to efficiently and quickly communicate insights with stakeholders.\n\n\n\nDr. Jonathan Keane is an engineering leader at Posit, PBC with a background in data science and social science. They have been building data tooling for 15 years, including both R and Python data tools for scientific and data science computing. They are a member of the PMC for Apache Arrow, a maintainer of the Apache Arrow package, and the author of dittodb. They have also worked as a data scientist in a number of different industries (identify verification and fraud, market research, call centers, and social justice among other areas) using a wide range of tools to analyze, model, and use data at large enterprise scales. On top of building data tooling, they have a passion for teaching data scientists and others how to use data and tools to do their work and inform their decisions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "materials/activities/03-practice-arrow.html#demo",
    "href": "materials/activities/03-practice-arrow.html#demo",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "materials/activities/03-practice-arrow.html#your-turn",
    "href": "materials/activities/03-practice-arrow.html#your-turn",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Your Turn",
    "text": "Your Turn"
  },
  {
    "objectID": "materials/activities/02-practice-datatable.html#demo",
    "href": "materials/activities/02-practice-datatable.html#demo",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "materials/activities/02-practice-datatable.html#your-turn",
    "href": "materials/activities/02-practice-datatable.html#your-turn",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Your Turn",
    "text": "Your Turn"
  },
  {
    "objectID": "materials/slides/03-arrow.html#section-overview",
    "href": "materials/slides/03-arrow.html#section-overview",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Section Overview",
    "text": "Section Overview\n\nIntroduction to Column-Oriented Data Storage \nDeep Dive into Parquet \nWorking with Arrow in R \nQuerying Parquet with Different Engines \nArrow Datasets for Larger-than-Memory Operations \nPartitioning Strategies \nHands-on Workshop: Analysis with PUMS Data"
  },
  {
    "objectID": "materials/slides/03-arrow.html#why-should-i-care-about-data-storage",
    "href": "materials/slides/03-arrow.html#why-should-i-care-about-data-storage",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Why should I care about data storage?",
    "text": "Why should I care about data storage?\n\nData has to be represented somewhere, both during analysis and when storing.\n\n\nThe shape and characteristics of this representation has a huge impact on performance.\n\n\nWhat if you could speed up a key part of your analysis by 30x and reduce your storage by 10x?"
  },
  {
    "objectID": "materials/slides/03-arrow.html#row-vs.-column-oriented-storage",
    "href": "materials/slides/03-arrow.html#row-vs.-column-oriented-storage",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Row vs. Column-Oriented Storage",
    "text": "Row vs. Column-Oriented Storage\n\n\nRow-oriented\n|ID|Name |Age|City    |\n|--|-----|---|--------|\n|1 |Alice|25 |New York|\n|2 |Bob  |30 |Boston  |\n|3 |Carol|45 |Chicago |\n\nEfficient for single record access\nEfficient for appending\n\n\n\nColumn-oriented\nID:    [1, 2, 3]\nName:  [Alice, Bob, Carol]\nAge:   [25, 30, 45]\nCity:  [New York, Boston, Chicago]\n\nEfficient for analytics\nBetter compression\n\n\n\n\nRow oriented formats are super familiar: CSVs as well as many databases\nBut Column-orientation isn’t something that is new and cutting edge. In fact, every single one of you use a system that stores data this way: R data frames(!)"
  },
  {
    "objectID": "materials/slides/03-arrow.html#why-column-oriented-storage",
    "href": "materials/slides/03-arrow.html#why-column-oriented-storage",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Why Column-Oriented Storage?",
    "text": "Why Column-Oriented Storage?\n\n\nAnalytics typically access a subset of columns\n\n“What is the average age by city?”\nOnly needs [Age, City] columns\n\nBenefits:\n\nOnly read needed columns from disk\nSimilar data types stored together\nBetter compression ratios\n\n\n\n\nCompression: this is because like-types are stored with like, so you get more frequent patterns — the core of compression. But you also can use encodings like dictionary encodings very efficiently."
  },
  {
    "objectID": "materials/slides/03-arrow.html#column-oriented-data-is-great",
    "href": "materials/slides/03-arrow.html#column-oriented-data-is-great",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Column-Oriented Data is great",
    "text": "Column-Oriented Data is great\n\nAnd you use column-oriented dataframes already!\n\n\n… but still storing my data in a fundamentally row-oriented way.\n\n\nThis isn’t so bad if you’re only talking about a small amount of data, transposing a few columns for a few rows is no big deal. But as data gets larger, or if you have to do this frequently, this process of transposing (AKA serialization) hurts."
  },
  {
    "objectID": "materials/slides/03-arrow.html#the-interconnection-problem",
    "href": "materials/slides/03-arrow.html#the-interconnection-problem",
    "title": "JSM 2025: Large Local Data Course",
    "section": "The interconnection problem",
    "text": "The interconnection problem\n\n\nMany of these were operating in essentially column-oriented ways — but to transfer data ended up writting into row-oriented data structures, then read them back in to something that was column-oriented.\nMoving data between representations is hard - Different formats, requirements, and limitations - Compatibility issues - Serialization is a huge bottleneck"
  },
  {
    "objectID": "materials/slides/03-arrow.html#the-interconnection-problem-1",
    "href": "materials/slides/03-arrow.html#the-interconnection-problem-1",
    "title": "JSM 2025: Large Local Data Course",
    "section": "The interconnection problem",
    "text": "The interconnection problem"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-apache-arrow",
    "href": "materials/slides/03-arrow.html#what-is-apache-arrow",
    "title": "JSM 2025: Large Local Data Course",
    "section": "What is Apache Arrow?",
    "text": "What is Apache Arrow?\n\n\n\nCross-language development platform for in-memory data\n\nConsistent in-memory columnar data format\nLanguage-independent\nZero-copy reads\n\nBenefits:\n\nSeamless data interchange between systems\nFast analytical processing\nEfficient memory usage"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-apache-parquet",
    "href": "materials/slides/03-arrow.html#what-is-apache-parquet",
    "title": "JSM 2025: Large Local Data Course",
    "section": "What is Apache Parquet?",
    "text": "What is Apache Parquet?\n\n\n\nOpen-source columnar storage format\n\nCreated by Twitter and Cloudera in 2013\nPart of the Apache Software Foundation\n\nFeatures:\n\nColumnar storage\nEfficient compression\nExplicit schema\nStatistical metadata"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-a-file",
    "href": "materials/slides/03-arrow.html#reading-a-file",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Reading a File",
    "text": "Reading a File\nAs a CSV file\n\nsystem.time({\n  df &lt;- read.csv(\"CA_person_2021.csv\")\n})\n\n\n   user  system elapsed \n 14.449   0.445  15.037 \n\n\nDescribe the CSV\nThis CSV is 708 MB, I’m reading this in on my MacBook Pro, your times will vary! We can use arrow or data.tables’s CSV reader and it’s faster (1.85 sec and 1.61 sec respectively). And if we read to an arrow table it’s even faster: 0.51 seconds"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-a-file-1",
    "href": "materials/slides/03-arrow.html#reading-a-file-1",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Reading a File",
    "text": "Reading a File\nAs a Parquet file\n\nlibrary(arrow)\noptions(arrow.use_altrep = FALSE)\n\nsystem.time({\n  df &lt;- read_parquet(\"CA_person_2021.parquet\")\n})\n\n\n   user  system elapsed \n  1.017   0.207   0.568 \n\n\nThe parquet file is 62 MB\nIt’s even faster with altrep (0.186 s), but that’s cheating! Also, if we read into an arrow table rather than a dataframe: 0.1 second"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-parquet",
    "href": "materials/slides/03-arrow.html#what-is-parquet",
    "title": "JSM 2025: Large Local Data Course",
    "section": "What is Parquet?",
    "text": "What is Parquet?\n\n\nSchema metadata\n\nSelf-describing format\nPreserves column types\nType-safe data interchange\n\nEncodings\n\nDictionary — Particularly effective for categorical data\nRun-length encoding - Efficient storage of sequential repeated values\n\nAdvanced compression\n\nColumn-specific compression algorithms\nBoth dictionary and value compression"
  },
  {
    "objectID": "materials/slides/03-arrow.html#exercise",
    "href": "materials/slides/03-arrow.html#exercise",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Exercise",
    "text": "Exercise\n\ndata &lt;- tibble::tibble(\n  integers = 1:10,\n  doubles = as.numeric(1:10),\n  strings = sprintf(\"%02d\", 1:10)\n)\n\nwrite.csv(data, \"numeric_base.csv\", row.names = FALSE)\nwrite_csv_arrow(data, \"numeric_arrow.csv\")\nwrite_parquet(data, \"numeric.parquet\")\n\ndf_csv &lt;- read.csv(\"numeric_base.csv\")\ndf_csv_arrow &lt;- read_csv_arrow(\"numeric_arrow.csv\")\ndf_parquet &lt;- read_parquet(\"numeric.parquet\")\n\n\nAre there any differences?"
  },
  {
    "objectID": "materials/slides/03-arrow.html#exercise-1",
    "href": "materials/slides/03-arrow.html#exercise-1",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Exercise",
    "text": "Exercise\n\n\n&gt; df_csv_arrow\n# A tibble: 10 × 3\n   integers doubles strings\n      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1        1       1       1\n 2        2       2       2\n 3        3       3       3\n 4        4       4       4\n 5        5       5       5\n 6        6       6       6\n 7        7       7       7\n 8        8       8       8\n 9        9       9       9\n10       10      10      10\n\n&gt; df_parquet\n# A tibble: 10 × 3\n   integers doubles strings\n      &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1       1 01     \n 2        2       2 02     \n 3        3       3 03     \n 4        4       4 04     \n 5        5       5 05     \n 6        6       6 06     \n 7        7       7 07     \n 8        8       8 08     \n 9        9       9 09     \n10       10      10 10"
  },
  {
    "objectID": "materials/slides/03-arrow.html#exercise-2",
    "href": "materials/slides/03-arrow.html#exercise-2",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Exercise",
    "text": "Exercise\n\n\n&gt; df_csv_arrow\n# A tibble: 10 × 3\n   integers doubles strings\n      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1        1       1       1\n 2        2       2       2\n 3        3       3       3\n 4        4       4       4\n 5        5       5       5\n 6        6       6       6\n 7        7       7       7\n 8        8       8       8\n 9        9       9       9\n10       10      10      10\n\n&gt; df_parquet\n# A tibble: 10 × 3\n   integers doubles strings\n      &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1       1 01     \n 2        2       2 02     \n 3        3       3 03     \n 4        4       4 04     \n 5        5       5 05     \n 6        6       6 06     \n 7        7       7 07     \n 8        8       8 08     \n 9        9       9 09     \n10       10      10 10"
  },
  {
    "objectID": "materials/slides/03-arrow.html#inside-a-parquet-file",
    "href": "materials/slides/03-arrow.html#inside-a-parquet-file",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Inside a Parquet File",
    "text": "Inside a Parquet File\n\n\n\nRow groups: Horizontal partitions of data\nColumn chunks: Columnar data within a row group\nPages: Small units of column chunk data\nFooter: Contains file metadata and schema"
  },
  {
    "objectID": "materials/slides/03-arrow.html#benchmarks-parquet-vs-csv",
    "href": "materials/slides/03-arrow.html#benchmarks-parquet-vs-csv",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Benchmarks: Parquet vs CSV",
    "text": "Benchmarks: Parquet vs CSV"
  },
  {
    "objectID": "materials/slides/03-arrow.html#benchmarks-parquet-vs-csv-1",
    "href": "materials/slides/03-arrow.html#benchmarks-parquet-vs-csv-1",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Benchmarks: Parquet vs CSV",
    "text": "Benchmarks: Parquet vs CSV"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-efficiency-selecting-columns",
    "href": "materials/slides/03-arrow.html#reading-efficiency-selecting-columns",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Reading Efficiency: Selecting Columns",
    "text": "Reading Efficiency: Selecting Columns\n\n\nWith CSV:\n\nMust read entire file, even if you only need a few columns\nNo efficient way to skip columns during read\n\nWith Parquet:\n\nRead only needed columns from disk\nSignificant performance benefit for wide tables\n\n\n\n\nsystem.time({\n  df_subset &lt;- read_parquet(\n    \"CA_person_2021.parquet\", \n    col_select = c(\"PUMA\", \"COW\")\n  )\n})\n\n\n\n\n   user  system elapsed \n  0.027   0.003   0.031 \n\n   user  system elapsed \n  1.017   0.207   0.568"
  },
  {
    "objectID": "materials/slides/03-arrow.html#nanoparquet-vs.-arrow-reader",
    "href": "materials/slides/03-arrow.html#nanoparquet-vs.-arrow-reader",
    "title": "JSM 2025: Large Local Data Course",
    "section": "nanoparquet vs. arrow Reader",
    "text": "nanoparquet vs. arrow Reader\n\n\nnanoparquet\n\nLightweight Parquet reader\nMinimal dependencies\nGood for embedding\n\narrow\n\nFull-featured reader\nSupport for datasets\nIntegration with Arrow ecosystem\nOptimized for analytics workloads"
  },
  {
    "objectID": "materials/slides/03-arrow.html#nanoparquet-vs.-arrow-reader-1",
    "href": "materials/slides/03-arrow.html#nanoparquet-vs.-arrow-reader-1",
    "title": "JSM 2025: Large Local Data Course",
    "section": "nanoparquet vs. arrow Reader",
    "text": "nanoparquet vs. arrow Reader\n\nlibrary(arrow)\noptions(arrow.use_altrep = FALSE)\n\nsystem.time({\n  df &lt;- read_parquet(\"CA_person_2021.parquet\")\n})\n\n   user  system elapsed \n  1.017   0.207   0.568 \n\n\n\n\n\nlibrary(nanoparquet)\n\nsystem.time({\n  df &lt;- read_parquet(\"CA_person_2021.parquet\")\n})\n\n   user  system elapsed \n  0.709   0.099   0.894"
  },
  {
    "objectID": "materials/slides/03-arrow.html#parquet-tooling-ecosystem",
    "href": "materials/slides/03-arrow.html#parquet-tooling-ecosystem",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Parquet Tooling Ecosystem",
    "text": "Parquet Tooling Ecosystem\nLanguages with native Parquet support:\n\n\nR (via arrow)\nPython (via pyarrow, pandas)\nJava\nC++\nRust\nJavaScript\nGo"
  },
  {
    "objectID": "materials/slides/03-arrow.html#parquet-tooling-ecosystem-1",
    "href": "materials/slides/03-arrow.html#parquet-tooling-ecosystem-1",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Parquet Tooling Ecosystem",
    "text": "Parquet Tooling Ecosystem\nSystems with Parquet integration:\n\n\nApache Spark\nApache Hadoop\nApache Drill\nSnowflake\nAmazon Athena\nGoogle BigQuery\nDuckDB"
  },
  {
    "objectID": "materials/slides/03-arrow.html#introduction-to-the-arrow-package",
    "href": "materials/slides/03-arrow.html#introduction-to-the-arrow-package",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Introduction to the arrow Package",
    "text": "Introduction to the arrow Package\n\n# Install and load the Arrow package\ninstall.packages(\"arrow\")\nlibrary(arrow)\n\n# Check Arrow version and capabilities\narrow_info()\n\n\n\nThe arrow package provides:\n\nNative R interface to Apache Arrow\nTools for working with large datasets\nIntegration with dplyr for data manipulation\nReading/writing various file formats"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-and-writing-parquet-files-revisited",
    "href": "materials/slides/03-arrow.html#reading-and-writing-parquet-files-revisited",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Reading and Writing Parquet files, revisited",
    "text": "Reading and Writing Parquet files, revisited\n\n# Read a Parquet file into R\ndata &lt;- read_parquet(\"CA_person_2021.parquet\")\n\n# Write an R data frame to Parquet\nwrite_parquet(data, \"CA_person_2021_new.parquet\")\n\n# Reading a subset of columns\ndf_subset &lt;- read_parquet(\n  \"CA_person_2021.parquet\", \n  col_select = c(\"PUMA\", \"COW\", \"AGEP\")\n)\n\n# Reading with a row filter (predicate pushdown)\ndf_filtered &lt;- open_dataset(\"CA_person_2021.parquet\") |&gt; \n  filter(AGEP &gt; 40) |&gt;\n  collect()"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-using-dplyr-with-arrow",
    "href": "materials/slides/03-arrow.html#demo-using-dplyr-with-arrow",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo: Using dplyr with arrow",
    "text": "Demo: Using dplyr with arrow\n\n# Create an Arrow Table\ndf &lt;- read_parquet(\"CA_person_2021.parquet\", as_data_frame = FALSE)\n\n# Use dplyr verbs with arrow tables\ndf |&gt;\n  filter(AGEP &gt;= 16) |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) \n\n\nThe dataframe is backed by altrep, actually. But generally functions like any other dataframe."
  },
  {
    "objectID": "materials/slides/03-arrow.html#introduction-to-duckdb",
    "href": "materials/slides/03-arrow.html#introduction-to-duckdb",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Introduction to DuckDB",
    "text": "Introduction to DuckDB\n\n\n\nAnalytical SQL database system\n\nEmbedded database (like SQLite)\nColumn oriented\nIn-process query execution\n\nKey features:\n\nDirect Parquet querying\nVectorized query execution\nParallel processing\nZero-copy integration with arrow\n\n\n\n\nThe zero-copy integration with arrow is because DuckDB uses basically the same format for it’s own internal representation."
  },
  {
    "objectID": "materials/slides/03-arrow.html#duckdb",
    "href": "materials/slides/03-arrow.html#duckdb",
    "title": "JSM 2025: Large Local Data Course",
    "section": "DuckDB",
    "text": "DuckDB\n\nlibrary(duckdb)\n\ncon &lt;- dbConnect(duckdb())\n\n# Register a Parquet file as a virtual table\ndbExecute(con, \"CREATE VIEW pums AS SELECT * \n                FROM read_parquet('CA_person_2021.parquet')\")\n\n# Run our query\ndbGetQuery(con, \"\n  SELECT SUM(JWMNP * PWGTP)/SUM(PWGTP) as avg_commute_time,\n         COUNT(*) as count\n  FROM pums\n  WHERE AGEP &gt;= 16\n\")\n\ndbDisconnect(con, shutdown = TRUE)"
  },
  {
    "objectID": "materials/slides/03-arrow.html#duckplyr",
    "href": "materials/slides/03-arrow.html#duckplyr",
    "title": "JSM 2025: Large Local Data Course",
    "section": "duckplyr",
    "text": "duckplyr\n\nlibrary(duckplyr)\n\n# Read data with Arrow\npums_data &lt;- read_file_duckdb(\n  \"CA_person_2021.parquet\", \n  \"read_parquet\"\n)\n\n# Use duckplyr to optimize dplyr operations\npums_data |&gt;\n  filter(AGEP &gt;= 16) |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |&gt;\n  collect()\n\n\nduckplyr is a drop-in replacement for dplyr, using duckdb as a backend"
  },
  {
    "objectID": "materials/slides/03-arrow.html#data.table",
    "href": "materials/slides/03-arrow.html#data.table",
    "title": "JSM 2025: Large Local Data Course",
    "section": "data.table",
    "text": "data.table\n\nlibrary(arrow)\nlibrary(data.table)\n\n# Read Parquet file with Arrow\npums_data &lt;- read_parquet(\"CA_person_2021.parquet\")\n\n# Convert to data.table\npums_dt &lt;- as.data.table(pums_data)\n\n# data.table query\npums_dt[AGEP &gt;= 16,\n  .(avg_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP), \n    count = .N)]"
  },
  {
    "objectID": "materials/slides/03-arrow.html#arrow-query-execution",
    "href": "materials/slides/03-arrow.html#arrow-query-execution",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Arrow Query Execution",
    "text": "Arrow Query Execution\n\n# Create an Arrow Dataset\npums_ds &lt;- open_dataset(\"pums_dataset_dir/\")\n\n# Execute query with Arrow\nresult &lt;- pums_ds |&gt;\n  filter(AGEP &gt;= 16) |&gt;\n  group_by(ST) |&gt;\n  summarize(\n    avg_commute_time = mean(JWMNP, na.rm = TRUE),\n    count = n()\n  ) |&gt;\n  arrange(desc(avg_commute_time)) |&gt;\n  collect()"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-seamless-integration-arrow-duckdb",
    "href": "materials/slides/03-arrow.html#demo-seamless-integration-arrow-duckdb",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo: Seamless Integration Arrow ↔︎ DuckDB",
    "text": "Demo: Seamless Integration Arrow ↔︎ DuckDB\n\ndf &lt;- read_parquet(\"CA_person_2021.parquet\")\n\n# Use dplyr verbs with arrow tables\ndf |&gt;\n  filter(AGEP &gt;= 16) |&gt;\n  to_duckdb() |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  )"
  },
  {
    "objectID": "materials/slides/03-arrow.html#understanding-arrow-datasets-vs.-tables",
    "href": "materials/slides/03-arrow.html#understanding-arrow-datasets-vs.-tables",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Understanding Arrow Datasets vs. Tables",
    "text": "Understanding Arrow Datasets vs. Tables\n\n\nArrow Table\n\nIn-memory data structure\nMust fit in RAM\nFast operations\nSimilar to base data frames\nGood for single file data\n\n\nArrow Dataset\n\nCollection of files\nLazily evaluated\nLarger-than-memory capable\nDistributed execution\nSupports partitioning"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-opening-and-querying-multi-file-datasets",
    "href": "materials/slides/03-arrow.html#demo-opening-and-querying-multi-file-datasets",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo: Opening and Querying Multi-file Datasets",
    "text": "Demo: Opening and Querying Multi-file Datasets\n\npums_ds &lt;- open_dataset(\"data/person\")\n\n# Examine the dataset, list files\nprint(pums_ds)\nhead(pums_ds$files)\n\n# Query execution with lazy evaluation\npums_ds |&gt;\n  filter(AGEP &gt;= 16) |&gt;\n  group_by(year, ST) |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |&gt;\n  collect()"
  },
  {
    "objectID": "materials/slides/03-arrow.html#lazy-evaluation-and-query-optimization",
    "href": "materials/slides/03-arrow.html#lazy-evaluation-and-query-optimization",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Lazy Evaluation and Query Optimization",
    "text": "Lazy Evaluation and Query Optimization\n\n\nLazy evaluation workflow:\n\nDefine operations (filter, group, summarize)\nArrow builds an execution plan\nOptimizes the plan (predicate pushdown, etc.)\nOnly reads necessary data from disk\nExecutes when collect() is called\n\nBenefits:\n\nMinimizes memory usage\nReduces I/O operations\nLeverages Arrow’s native compute functions"
  },
  {
    "objectID": "materials/slides/03-arrow.html#working-with-datasets-on-s3",
    "href": "materials/slides/03-arrow.html#working-with-datasets-on-s3",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Working with Datasets on S3",
    "text": "Working with Datasets on S3\narrow can work with data and datasets in cloud storage. This can be a good option if you don’t have access to a formal DBMS.\n\n\nEasy to store\narrow efficiently uses metadata to read only what is necessary\n\n\n\nI know, I know — this workshop is about local files. But I couldn’t help myself"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-working-with-datasets-on-s3",
    "href": "materials/slides/03-arrow.html#demo-working-with-datasets-on-s3",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo: Working with Datasets on S3",
    "text": "Demo: Working with Datasets on S3\n\npums_ds &lt;- open_dataset(\"s3://scaling-arrow-pums/person/\")\n\n# Query execution with lazy evaluation\npums_ds |&gt;\n  filter(year == 2021, location == \"ca\", AGEP &gt;= 16) |&gt;\n  group_by(year, ST) |&gt;\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |&gt;\n  collect()\n\n\nTalk about partitioning helping, etc"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-sipping-data",
    "href": "materials/slides/03-arrow.html#demo-sipping-data",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo: Sipping data",
    "text": "Demo: Sipping data\n\npums_ds &lt;- open_dataset(\"s3://scaling-arrow-pums/person/\")\n\n# Query execution with lazy evaluation\npums_ds |&gt;\n  filter(AGEP &gt;= 97) |&gt;\n  collect()\n\n\nSys.getpid() nettop -p X"
  },
  {
    "objectID": "materials/slides/03-arrow.html#what-is-partitioning",
    "href": "materials/slides/03-arrow.html#what-is-partitioning",
    "title": "JSM 2025: Large Local Data Course",
    "section": "What is Partitioning?",
    "text": "What is Partitioning?\n\n\nDividing data into logical segments\n\nStored in separate files/directories\nBased on one or more column values\nEnables efficient filtering\n\nBenefits:\n\nFaster queries that filter on partition columns\nImproved parallel processing\nEasier management of large datasets"
  },
  {
    "objectID": "materials/slides/03-arrow.html#hive-vs.-non-hive-partitioning",
    "href": "materials/slides/03-arrow.html#hive-vs.-non-hive-partitioning",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Hive vs. Non-Hive Partitioning",
    "text": "Hive vs. Non-Hive Partitioning\n\n\nHive Partitioning\n\nDirectory format: column=value\nExample:\nperson/\n├── year=2018/\n│   ├── state=NY/\n│   │   └── data.parquet\n│   └── state=CA/\n│       └── data.parquet\n├── year=2019/\n│   ├── ...\nSelf-describing structure\nStandard in big data ecosystem\n\n\nNon-Hive Partitioning\n\nDirectory format: value\nExample:\nperson/\n├── 2018/\n│   ├── NY/\n│   │   └── data.parquet\n│   └── CA/\n│       └── data.parquet\n├── 2019/\n│   ├── ...\nRequires column naming\nLess verbose directory names"
  },
  {
    "objectID": "materials/slides/03-arrow.html#effective-partitioning-strategies",
    "href": "materials/slides/03-arrow.html#effective-partitioning-strategies",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Effective Partitioning Strategies",
    "text": "Effective Partitioning Strategies\n\n\nChoose partition columns wisely:\n\nLow to medium cardinality\nCommonly used in filters\nBalanced data distribution\n\nCommon partition dimensions:\n\nTime (year, month, day)\nGeography (country, state, region)\nCategory (product type, department)\nSource (system, sensor)"
  },
  {
    "objectID": "materials/slides/03-arrow.html#partitioning-in-practice-writing-datasets",
    "href": "materials/slides/03-arrow.html#partitioning-in-practice-writing-datasets",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Partitioning in Practice: Writing Datasets",
    "text": "Partitioning in Practice: Writing Datasets\n\nca_pums_data &lt;- read_parquet(\"CA_person_2021.parquet\")\n\nca_pums_data |&gt;\n  mutate(\n    age_group = case_when(\n      AGEP &lt; 18 ~ \"under_18\",\n      AGEP &lt; 30 ~ \"18_29\",\n      AGEP &lt; 45 ~ \"30_44\",\n      AGEP &lt; 65 ~ \"45_64\",\n      TRUE ~ \"65_plus\"\n    )\n  ) |&gt;\n  group_by(ST, age_group) |&gt;\n  write_dataset(\n    path = \"ca_pums_by_age/\"\n  )"
  },
  {
    "objectID": "materials/slides/03-arrow.html#demo-repartitioning-the-whole-dataset",
    "href": "materials/slides/03-arrow.html#demo-repartitioning-the-whole-dataset",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo: repartitioning the whole dataset",
    "text": "Demo: repartitioning the whole dataset\n\npums_data &lt;- open_dataset(\"data/person\")\n\npums_data |&gt;\n  mutate(\n    age_group = case_when(\n      AGEP &lt; 18 ~ \"under_18\",\n      AGEP &lt; 30 ~ \"18_29\",\n      AGEP &lt; 45 ~ \"30_44\",\n      AGEP &lt; 65 ~ \"45_64\",\n      TRUE ~ \"65_plus\"\n    )\n  ) |&gt;\n  group_by(year, ST, age_group) |&gt;\n  write_dataset(\n    path = \"pums_by_age/\"\n  )"
  },
  {
    "objectID": "materials/slides/03-arrow.html#best-practices-for-partition-design",
    "href": "materials/slides/03-arrow.html#best-practices-for-partition-design",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Best Practices for Partition Design",
    "text": "Best Practices for Partition Design\n\n\nAvoid over-partitioning:\n\nToo many small files = poor performance\nTarget file size: 20MB–2GB\nAvoid high-cardinality columns (e.g., user_id)\n\nConsider query patterns:\n\nPartition by commonly filtered columns\nBalance between read speed and write complexity\n\nNested partitioning considerations:\n\nOrder from highest to lowest selectivity\nLimit partition depth (2-3 levels typically sufficient)"
  },
  {
    "objectID": "materials/slides/03-arrow.html#partitioning-performance-impact",
    "href": "materials/slides/03-arrow.html#partitioning-performance-impact",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Partitioning Performance Impact",
    "text": "Partitioning Performance Impact\n\n\nopen_dataset(\"&lt;path/to/data&gt;\") |&gt;\n  filter(year &gt;= 2018) |&gt;\n  summarise(\n    mean_commute = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP)\n  ) |&gt;\n  collect()"
  },
  {
    "objectID": "materials/slides/03-arrow.html#the-pums-dataset",
    "href": "materials/slides/03-arrow.html#the-pums-dataset",
    "title": "JSM 2025: Large Local Data Course",
    "section": "The PUMS Dataset",
    "text": "The PUMS Dataset\n\n\nPublic Use Microdata Sample\n\nUS Census Bureau data\nIndividual person and household records\nAnonymized demographic information\nIncome, education, housing, commute, etc.\n\nDataset characteristics:\n\nMultiple years (2005-2022)\nAll US states and territories\n53 Million rows (person only)\n200+ variables"
  },
  {
    "objectID": "materials/slides/03-arrow.html#reading-partitioned-pums-data",
    "href": "materials/slides/03-arrow.html#reading-partitioned-pums-data",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Reading Partitioned PUMS Data",
    "text": "Reading Partitioned PUMS Data\n\nlibrary(arrow)\nlibrary(dplyr)\n\n# Open the PUMS dataset\npums_path &lt;- \"data/person\"  # Partitioned by year and location\npums_ds &lt;- open_dataset(pums_path)\n\n# Examine the dataset\npums_ds\n\n# Look at the first few file paths\nhead(pums_ds$files)\n\n# Examine schema\npums_ds$schema"
  },
  {
    "objectID": "materials/slides/03-arrow.html#exercise-1-basic-filtering-and-aggregation",
    "href": "materials/slides/03-arrow.html#exercise-1-basic-filtering-and-aggregation",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Exercise 1: Basic Filtering and Aggregation",
    "text": "Exercise 1: Basic Filtering and Aggregation\n\n# Calculate average income by state for 2021\nresult1 &lt;- pums_ds |&gt;\n  filter(year == 2021) |&gt;\n  filter(PINCP &gt; 0) |&gt;  # Positive income only\n  group_by(location) |&gt;\n  summarize(\n    avg_income = mean(PINCP, na.rm = TRUE),\n    median_income = quantile(PINCP, 0.5, na.rm = TRUE),\n    n = n()\n  ) |&gt;\n  arrange(desc(avg_income)) |&gt;\n  collect()\n\n# View results\nhead(result1)"
  },
  {
    "objectID": "materials/slides/03-arrow.html#exercise-2-using-duckdb-with-pums-data",
    "href": "materials/slides/03-arrow.html#exercise-2-using-duckdb-with-pums-data",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Exercise 2: Using DuckDB with PUMS Data",
    "text": "Exercise 2: Using DuckDB with PUMS Data\n\nlibrary(arrow)\nlibrary(duckdb)\nlibrary(DBI)\n\n# Open the PUMS dataset with Arrow\npums_ds &lt;- open_dataset(\"data/person\")\n\n# Filter to just 2021 data for Washington\nwa_2021 &lt;- pums_ds |&gt;\n  filter(year == 2021, location == \"wa\") |&gt;\n  collect()\n\n# Create DuckDB connection\ncon &lt;- dbConnect(duckdb())\n\n# Register Arrow Table with DuckDB\nduckdb::duckdb_register_arrow(con, \"pums_wa\", wa_2021)\n\n# Run SQL query\nresult &lt;- dbGetQuery(con, \"\n  SELECT \n    CASE \n      WHEN AGEP &lt; 18 THEN 'Under 18'\n      WHEN AGEP &lt; 30 THEN '18-29'\n      WHEN AGEP &lt; 45 THEN '30-44'\n      WHEN AGEP &lt; 65 THEN '45-64'\n      ELSE '65+'\n    END AS age_group,\n    AVG(JWMNP) AS avg_commute_time,\n    COUNT(*) AS n\n  FROM pums_wa\n  WHERE JWMNP &gt; 0\n  GROUP BY age_group\n  ORDER BY age_group\n\")\n\n# Disconnect when done\ndbDisconnect(con, shutdown = TRUE)"
  },
  {
    "objectID": "materials/slides/03-arrow.html#challenge-formulate-your-own-analysis",
    "href": "materials/slides/03-arrow.html#challenge-formulate-your-own-analysis",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Challenge: Formulate Your Own Analysis",
    "text": "Challenge: Formulate Your Own Analysis\n\n\nChoose a research question:\n\nHow has commute time changed over the years?\nWhat’s the relationship between education and income?\nHow does housing cost burden vary by state?\nYour own question…\n\nImplement the analysis using:\n\nArrow Dataset operations\nDuckDB SQL queries\nData visualization\n\nCompare performance between approaches"
  },
  {
    "objectID": "materials/slides/03-arrow.html#conclusion",
    "href": "materials/slides/03-arrow.html#conclusion",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nColumn-oriented storage formats like Parquet provide massive performance advantages for analytical workloads (30x speed, 10x smaller files)\nApache Arrow enables seamless data interchange between systems without costly serialization/deserialization\nMultiple query engines (arrow, DuckDB, data.table) offer flexibility depending on your analysis needs, all using modern formats like Parquet\nPartitioning strategies help manage large datasets effectively when working with data too big for memory"
  },
  {
    "objectID": "materials/slides/03-arrow.html#conclusion-1",
    "href": "materials/slides/03-arrow.html#conclusion-1",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Conclusion",
    "text": "Conclusion\nResources:\n\nWorkshop materials: GitHub Repository\nArrow documentation: arrow.apache.org/docs/r\nParquet: parquet.apache.org\nDuckDB: duckdb.org\nBook: Scaling up with Arrow and R\n\nQuestions?"
  },
  {
    "objectID": "materials/activities/04-practice-workflow.html#demo",
    "href": "materials/activities/04-practice-workflow.html#demo",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "materials/activities/04-practice-workflow.html#your-turn",
    "href": "materials/activities/04-practice-workflow.html#your-turn",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Your Turn",
    "text": "Your Turn"
  },
  {
    "objectID": "materials/activities/01-practice-data.html#demo",
    "href": "materials/activities/01-practice-data.html#demo",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "materials/activities/01-practice-data.html#your-turn",
    "href": "materials/activities/01-practice-data.html#your-turn",
    "title": "JSM 2025: Large Local Data Course",
    "section": "Your Turn",
    "text": "Your Turn"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JSM 2025 Short Course",
    "section": "",
    "text": "by: Kelly Bodwin, Tyson Barrett & Jon Keane"
  },
  {
    "objectID": "index.html#large-local-data",
    "href": "index.html#large-local-data",
    "title": "JSM 2025 Short Course",
    "section": "",
    "text": "by: Kelly Bodwin, Tyson Barrett & Jon Keane"
  },
  {
    "objectID": "index.html#short-course-description",
    "href": "index.html#short-course-description",
    "title": "JSM 2025 Short Course",
    "section": "Short Course Description",
    "text": "Short Course Description\nIt is increasingly common in academic and professional settings to encounter datasets large enough to exceed the capabilities of standard data processing tools, yet small enough to be stored on local computers. Recent articles even claim that “the era of big data is over” and that data analysts and researchers should “think small, develop locally, ship joyfully” Such “medium” dataests are instrumental in measuring, tracking, and recording a wide array of phenomena across disciplines such as human behavior, animal studies, geology, economics, and astronomy. In this workshop, we will present modern techniques for handling large local data in R using a tidy data pipeline, encompassing stages from data storage and importing to cleaning, analysis, and exporting data and analyses. Specifically, we will teach a combination of tools from the data.table, arrow, and duckDB packages, with a focus on parquet data files for storage and transfer. By the end of the workshop, participants will understand how to integrate these tools to establish a legible, reproducible, efficient, and high-performance workflow."
  },
  {
    "objectID": "index.html#intended-audience-and-level",
    "href": "index.html#intended-audience-and-level",
    "title": "JSM 2025 Short Course",
    "section": "Intended Audience and Level",
    "text": "Intended Audience and Level\nWe expect attendees to have R fluency at the level of a typical introductory course, such as the textbook R for Data Science (Wickham, Çetinkaya-Rundel, & Grolemund 2023); as well as familiarity with some data application that may motivate tools beyond the introductory level."
  },
  {
    "objectID": "dev/Outline_draft.html",
    "href": "dev/Outline_draft.html",
    "title": "Course Outline",
    "section": "",
    "text": "How big is my data? Long, wide or both?\nHow big is my analysis? Big because many groups? Big because looping? Big because matrix computation?\nHow often will I repeat my analysis? By myself as I refine it? In production?\nHow to choose a tool? -&gt; You have one dataset. It can read into R, but tasks are slow:\ngrouping and summarizing, esp over a variable with many cats\nmapping through a column, e.g to string process or custom function\ncomputing lags, sliders, etc.\nmaking many new columns\n\nWhat’s my slowdown? -&gt; profvis\nPersona: Survey research\n-&gt; You have one dataset. It is too large to read into R.\nPersona: Biological genetic data\nWhat’s my slowdown? -&gt; read a few lines, develop your pipeline, profvis\n-&gt; You have many datasets. At least one is too large to read into R. Joining and subsetting is slow.\nPersona: Customer data\n\nWhat makes some code faster than others?\nEfficiency of pipeline: filter before mutate.\nEfficiency of algorithm: think matrix order\nEfficiency of memory handling: careful C allocations\nEfficiency of interpreting or compiling\nEfficiency of data storage structure\nSmart saving of intermediate objects.\n\nx: data size y: number of repeated operations\n1,1: any 1,2: data table. [write more efficient code] 2,1: parquet & arrow [use better storage formats] 2,2: duckdb & arrow [run in SQL not R]\nhigher x -&gt; cloud database higher y -&gt; bigger machine/parallelize across machines/whatever\n\n\n\n\n\n\nsmall: &lt; 1000 or something, everything is split second\nmedium: 100000 or so, and/or many categories, can read into R but analysis is slow\nlarge: parquet can store on local machine\nmassive: too big for local machine\nQuestions for Tyson/Jon: - Easy way to convert csv to parquet w/o reading into R?\n- Column and row size; handled differently in R? In parquet? In a db?\nTitle of course Storing, Importing, Managing, and Analyzing Large Data Locally with R Instructor 1 Kelly Bodwin Instructor 1 Email kbodwin@calpoly.edu Instructor 2 Tyson Barrett Instructor 2 Email tyson.barrett@usu.edu Instructor 3 Jonathan Keane Instructor 3 Email jkeane@gmail.com Length of Course\nFull-day (7.5 contact hours) Course Description It is increasingly common in academic and professional settings to encounter datasets large enough to exceed the capabilities of standard data processing tools, yet small enough to be stored on local computers. Recent articles even claim that “the era of big data is over” and that data analysts and researchers should “think small, develop locally, ship joyfully.” Such “medium” dataests are instrumental in measuring, tracking, and recording a wide array of phenomena across disciplines such as human behavior, animal studies, geology, economics, and astronomy. In this workshop, we will present modern techniques for handling large local data in R using a tidy data pipeline, encompassing stages from data storage and importing to cleaning, analysis, and exporting data and analyses. Specifically, we will teach a combination of tools from the data.table, arrow, and duckDB packages, with a focus on parquet data files for storage and transfer. By the end of the workshop, participants will understand how to integrate these tools to establish a legible, reproducible, efficient, and high-performance workflow.\nCourse Outline The following outline shows our planned approach to managing and analyzing large data locally in R. Our target audience are individuals in academic or professional data analysis positions, who work regularly with datasets that are manageable in terms of local storage but pose significant challenges in processing and cleaning due to their size and complexity.\nUnit 1: Identifying slowdowns in your local data process (Bodwin; 1 hour)\n1.1 Finding the problem:\n- User-friendly code timing with tictoc - Comparing runtimes with atime - Code profiling with profvis\n1.2 Categories of bottlenecks - Common scenarios for repeated runs of code sections - Speed impact from order-of-operations in data wrangling - Fast vs. slow types of dataset operations in R\n1.3 Activity: Code-along demo - Walkthrough of common data structures and tasks that could benefit from modern large-data tools\nUnit 2: In-Memory data wrangling with data.table (Barrett; 2 hours)\n2.1 Introduction - Basic syntax and structure of data.table - Speed comparison for common simple data tasks - High-level, user-friendly intuition for data.table’s “under the hood” parallel processing and C optimization\n2.2 Data wrangling tools - Filtering, summarizing, grouping, and mutating data - Sophisticated data processing with the set* functions.\n2.3 Activity: Code-Along - Real-data examples of data.table use for processing and analyzing data.\n2.4 Reference semantics - Speed and memory gains from modify-by-reference - Effects and side-effects of modify-by-reference - data.table syntax for fast no-copy data transformation\n2.5 Helper packages - Brief highlight of dtplyr and tidyfast as syntactical wrappers to data.table - Storing and reading data.table objects with parquet.\n2.6 Activity: Case Study - Learners work through a guided but incomplete real-data analysis.\nUnit 3: Storing, Reading, and Converting data with arrow, parquet, and duckdb (Keane 2 hours, Bodwin/Barrett 1 hour)\n3.1 Introduction to Arrow and Parquet - Intro to history and development of Arrow - Basic Arrow infrastructure and syntax - Discussion of the interchange problem - Using arrow reader and nanoparquet for efficient dataset storage and input. - Discussion of the Parquet structure, including column orientation and its benefits\n3.2 Activity: Code-Along - Data analysis with Arrow.\n3.3 Introduction to DuckDB - Introduction to duckDB and the local database model. - Basic duckDB syntax. - Data processing and analysis in duckDB - Helper packages such as duckplyr. - Working with duckDB and parquet files simultaneously.\n3.4 Activity: Code-Along - Data analysis with duckDB.\n3.5 Comparison of tools - Similarities and trade-offs of arrow, duckDB, and data.table. - Options for dplyr syntax in all three packages.\n3.6 Activity: Case Study - Goals: Compare, contrast, and benchmark - Learners repeat a data analysis task three times, using each of the three tools. - Learners benchmark the speed of each step of the task in the three implementations. - Discussion and reflection on learner-preferred syntax and usage.\nUnit 4: Putting it together: a workflow for efficient data manipulation (Bodwin/Barrett 1.5 hours)\n4.1 Showcase: A tidy pipeline using these modern, efficient tools - Import/export: fread, parquet with arrow/duckDB - Tidy: dtplyr, duckplyr, arrow - Transform: dtplyr, duckplyr, arrow\n4.2 Decisions and Guidelines - When to choose fread with csv versus parquet conversion. - Pros and cons of the local database structure versus local raw data files. - Specific data sizes, formats, and computations that are best suited to each tool.\n4.3 Activity: Final Case Study - Learners take ownership of a case study of real world large data, writing their own code with a large dataset from start to finish with instructor support Learning Objectives Diagnosing and Benchmarking: - Incorporate time checks into a data analysis workflow to identify slowdowns. - Recognize workflow sections that are likely to be re-run - Design data pipeline steps to isolate and improve bottlenecks\nSyntax: - Write basic data analysis code in data.table - Write basic data analysis code in arrow - Write basic data analysis code in duckDB - Write dplyr syntax code with dtplyr and duckplyr\nConcepts and Ideas: - Recognize grouping and summarizing operations that will benefit from the data.table implementation - Understand the modify-by-reference approach - Understand the benefits of parquet’s column orientation storage - Know the difference between a collection of local files and a local database.\nWorkflow: - Read csv data with fread and parquet format data with arrow - Set up and read/write data in a local duckDB database - Smoothly switch between major large data tools within a single data processing and analyzing pipeline. Instructor(s) Background Dr. Kelly Bodwin is an educator with over a decade of experience of teaching statistics and data science with R. She has co-authored multiple R packages, including flair and tidyclust, and she is currently a consultant on an NSF Grant building infrastructure for the data.table package. Her published applied research frequently involves manipulating large, in-memory data. Examples include: performing large matrix computations on high-dimensional GWAS (genome-wide association studies) data; constructing temporal social networks at hundreds of time checkpoints for organizational membership data; and summarizing biodiversity metrics grouped over exhaustive permutations of taxa level organism counts and experimental conditions. Above all, Dr. Bodwin’s educational goal is to lower barriers to entry for beginner and intermediate R users to benefit from modern tools and enable more efficient and effective data workflows.\nDr. Tyson Barrett is a researcher and an applied statistician at Highmark Health and Utah State University. He has over 15 years of R package development and programming experience, including maintaining data.table (with over 600,000 monthly downloads) and 3 other published R packages. He is currently a consultant on an NSF Grant building infrastructure for the data.table package. In his research work, he regularly works with large datasets with millions of rows and hundreds of columns. He and his team use data.table, arrow, and duckDB daily to manage and analyze their data to efficiently and quickly communicate insights with stakeholders.\nDr. Jonathan Keane is an engineering leader at Posit, PBC with a background in data science and social science. They have been building data tooling for 15 years, including both R and Python data tools for scientific and data science computing. They are a member of the PMC for Apache Arrow, a maintainer of the Apache Arrow package, and the author of dittodb. They have also worked as a data scientist in a number of different industries (identify verification and fraud, market research, call centers, and social justice among other areas) using a wide range of tools to analyze, model, and use data at large enterprise scales. On top of building data tooling, they have a passion for teaching data scientists and others how to use data and tools to do their work and inform their decisions.\nAdditional Comments Learners should bring a working laptop with an installation of R 4.0+ and a recent (2023 or later) installation of RStudio or Positron. Learners should ensure that their laptop has admin permission for installation of new R packages.\nA beginner-intermediate level of working knowledge in R with the tidyverse is assumed; at approximately the level of Chapters 1-8 in Wickham’s R for Data Science (2e)."
  },
  {
    "objectID": "dev/Outline_draft.html#unit-one-defining-the-problem",
    "href": "dev/Outline_draft.html#unit-one-defining-the-problem",
    "title": "Course Outline",
    "section": "",
    "text": "How big is my data? Long, wide or both?\nHow big is my analysis? Big because many groups? Big because looping? Big because matrix computation?\nHow often will I repeat my analysis? By myself as I refine it? In production?\nHow to choose a tool? -&gt; You have one dataset. It can read into R, but tasks are slow:\ngrouping and summarizing, esp over a variable with many cats\nmapping through a column, e.g to string process or custom function\ncomputing lags, sliders, etc.\nmaking many new columns\n\nWhat’s my slowdown? -&gt; profvis\nPersona: Survey research\n-&gt; You have one dataset. It is too large to read into R.\nPersona: Biological genetic data\nWhat’s my slowdown? -&gt; read a few lines, develop your pipeline, profvis\n-&gt; You have many datasets. At least one is too large to read into R. Joining and subsetting is slow.\nPersona: Customer data\n\nWhat makes some code faster than others?\nEfficiency of pipeline: filter before mutate.\nEfficiency of algorithm: think matrix order\nEfficiency of memory handling: careful C allocations\nEfficiency of interpreting or compiling\nEfficiency of data storage structure\nSmart saving of intermediate objects.\n\nx: data size y: number of repeated operations\n1,1: any 1,2: data table. [write more efficient code] 2,1: parquet & arrow [use better storage formats] 2,2: duckdb & arrow [run in SQL not R]\nhigher x -&gt; cloud database higher y -&gt; bigger machine/parallelize across machines/whatever"
  },
  {
    "objectID": "dev/Outline_draft.html#unit-three-handle-it-in-a-local-database-with-arrow-and-duckdb",
    "href": "dev/Outline_draft.html#unit-three-handle-it-in-a-local-database-with-arrow-and-duckdb",
    "title": "Course Outline",
    "section": "",
    "text": "small: &lt; 1000 or something, everything is split second\nmedium: 100000 or so, and/or many categories, can read into R but analysis is slow\nlarge: parquet can store on local machine\nmassive: too big for local machine\nQuestions for Tyson/Jon: - Easy way to convert csv to parquet w/o reading into R?\n- Column and row size; handled differently in R? In parquet? In a db?\nTitle of course Storing, Importing, Managing, and Analyzing Large Data Locally with R Instructor 1 Kelly Bodwin Instructor 1 Email kbodwin@calpoly.edu Instructor 2 Tyson Barrett Instructor 2 Email tyson.barrett@usu.edu Instructor 3 Jonathan Keane Instructor 3 Email jkeane@gmail.com Length of Course\nFull-day (7.5 contact hours) Course Description It is increasingly common in academic and professional settings to encounter datasets large enough to exceed the capabilities of standard data processing tools, yet small enough to be stored on local computers. Recent articles even claim that “the era of big data is over” and that data analysts and researchers should “think small, develop locally, ship joyfully.” Such “medium” dataests are instrumental in measuring, tracking, and recording a wide array of phenomena across disciplines such as human behavior, animal studies, geology, economics, and astronomy. In this workshop, we will present modern techniques for handling large local data in R using a tidy data pipeline, encompassing stages from data storage and importing to cleaning, analysis, and exporting data and analyses. Specifically, we will teach a combination of tools from the data.table, arrow, and duckDB packages, with a focus on parquet data files for storage and transfer. By the end of the workshop, participants will understand how to integrate these tools to establish a legible, reproducible, efficient, and high-performance workflow.\nCourse Outline The following outline shows our planned approach to managing and analyzing large data locally in R. Our target audience are individuals in academic or professional data analysis positions, who work regularly with datasets that are manageable in terms of local storage but pose significant challenges in processing and cleaning due to their size and complexity.\nUnit 1: Identifying slowdowns in your local data process (Bodwin; 1 hour)\n1.1 Finding the problem:\n- User-friendly code timing with tictoc - Comparing runtimes with atime - Code profiling with profvis\n1.2 Categories of bottlenecks - Common scenarios for repeated runs of code sections - Speed impact from order-of-operations in data wrangling - Fast vs. slow types of dataset operations in R\n1.3 Activity: Code-along demo - Walkthrough of common data structures and tasks that could benefit from modern large-data tools\nUnit 2: In-Memory data wrangling with data.table (Barrett; 2 hours)\n2.1 Introduction - Basic syntax and structure of data.table - Speed comparison for common simple data tasks - High-level, user-friendly intuition for data.table’s “under the hood” parallel processing and C optimization\n2.2 Data wrangling tools - Filtering, summarizing, grouping, and mutating data - Sophisticated data processing with the set* functions.\n2.3 Activity: Code-Along - Real-data examples of data.table use for processing and analyzing data.\n2.4 Reference semantics - Speed and memory gains from modify-by-reference - Effects and side-effects of modify-by-reference - data.table syntax for fast no-copy data transformation\n2.5 Helper packages - Brief highlight of dtplyr and tidyfast as syntactical wrappers to data.table - Storing and reading data.table objects with parquet.\n2.6 Activity: Case Study - Learners work through a guided but incomplete real-data analysis.\nUnit 3: Storing, Reading, and Converting data with arrow, parquet, and duckdb (Keane 2 hours, Bodwin/Barrett 1 hour)\n3.1 Introduction to Arrow and Parquet - Intro to history and development of Arrow - Basic Arrow infrastructure and syntax - Discussion of the interchange problem - Using arrow reader and nanoparquet for efficient dataset storage and input. - Discussion of the Parquet structure, including column orientation and its benefits\n3.2 Activity: Code-Along - Data analysis with Arrow.\n3.3 Introduction to DuckDB - Introduction to duckDB and the local database model. - Basic duckDB syntax. - Data processing and analysis in duckDB - Helper packages such as duckplyr. - Working with duckDB and parquet files simultaneously.\n3.4 Activity: Code-Along - Data analysis with duckDB.\n3.5 Comparison of tools - Similarities and trade-offs of arrow, duckDB, and data.table. - Options for dplyr syntax in all three packages.\n3.6 Activity: Case Study - Goals: Compare, contrast, and benchmark - Learners repeat a data analysis task three times, using each of the three tools. - Learners benchmark the speed of each step of the task in the three implementations. - Discussion and reflection on learner-preferred syntax and usage.\nUnit 4: Putting it together: a workflow for efficient data manipulation (Bodwin/Barrett 1.5 hours)\n4.1 Showcase: A tidy pipeline using these modern, efficient tools - Import/export: fread, parquet with arrow/duckDB - Tidy: dtplyr, duckplyr, arrow - Transform: dtplyr, duckplyr, arrow\n4.2 Decisions and Guidelines - When to choose fread with csv versus parquet conversion. - Pros and cons of the local database structure versus local raw data files. - Specific data sizes, formats, and computations that are best suited to each tool.\n4.3 Activity: Final Case Study - Learners take ownership of a case study of real world large data, writing their own code with a large dataset from start to finish with instructor support Learning Objectives Diagnosing and Benchmarking: - Incorporate time checks into a data analysis workflow to identify slowdowns. - Recognize workflow sections that are likely to be re-run - Design data pipeline steps to isolate and improve bottlenecks\nSyntax: - Write basic data analysis code in data.table - Write basic data analysis code in arrow - Write basic data analysis code in duckDB - Write dplyr syntax code with dtplyr and duckplyr\nConcepts and Ideas: - Recognize grouping and summarizing operations that will benefit from the data.table implementation - Understand the modify-by-reference approach - Understand the benefits of parquet’s column orientation storage - Know the difference between a collection of local files and a local database.\nWorkflow: - Read csv data with fread and parquet format data with arrow - Set up and read/write data in a local duckDB database - Smoothly switch between major large data tools within a single data processing and analyzing pipeline. Instructor(s) Background Dr. Kelly Bodwin is an educator with over a decade of experience of teaching statistics and data science with R. She has co-authored multiple R packages, including flair and tidyclust, and she is currently a consultant on an NSF Grant building infrastructure for the data.table package. Her published applied research frequently involves manipulating large, in-memory data. Examples include: performing large matrix computations on high-dimensional GWAS (genome-wide association studies) data; constructing temporal social networks at hundreds of time checkpoints for organizational membership data; and summarizing biodiversity metrics grouped over exhaustive permutations of taxa level organism counts and experimental conditions. Above all, Dr. Bodwin’s educational goal is to lower barriers to entry for beginner and intermediate R users to benefit from modern tools and enable more efficient and effective data workflows.\nDr. Tyson Barrett is a researcher and an applied statistician at Highmark Health and Utah State University. He has over 15 years of R package development and programming experience, including maintaining data.table (with over 600,000 monthly downloads) and 3 other published R packages. He is currently a consultant on an NSF Grant building infrastructure for the data.table package. In his research work, he regularly works with large datasets with millions of rows and hundreds of columns. He and his team use data.table, arrow, and duckDB daily to manage and analyze their data to efficiently and quickly communicate insights with stakeholders.\nDr. Jonathan Keane is an engineering leader at Posit, PBC with a background in data science and social science. They have been building data tooling for 15 years, including both R and Python data tools for scientific and data science computing. They are a member of the PMC for Apache Arrow, a maintainer of the Apache Arrow package, and the author of dittodb. They have also worked as a data scientist in a number of different industries (identify verification and fraud, market research, call centers, and social justice among other areas) using a wide range of tools to analyze, model, and use data at large enterprise scales. On top of building data tooling, they have a passion for teaching data scientists and others how to use data and tools to do their work and inform their decisions.\nAdditional Comments Learners should bring a working laptop with an installation of R 4.0+ and a recent (2023 or later) installation of RStudio or Positron. Learners should ensure that their laptop has admin permission for installation of new R packages.\nA beginner-intermediate level of working knowledge in R with the tidyverse is assumed; at approximately the level of Chapters 1-8 in Wickham’s R for Data Science (2e)."
  }
]