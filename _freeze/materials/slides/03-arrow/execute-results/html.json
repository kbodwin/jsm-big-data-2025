{
  "hash": "6d740e1f9d2f96d2facd52b7ecc3c143",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"parquet, arrow, duckdb\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    footer: \"[JSM: Large Data](https://github.com/kbodwin/jsm-large-data)\"\n    theme: simple\n    # scrollable: true\n    embed-resources: true\n    include-in-header:\n      - text: |\n          <style>\n          .extrapad {\n              margin-top:1em  !important;\n          }\n          </style>\neditor: source\n---\n\n\n## Section Overview\n\n1. **Introduction to Column-Oriented Data Storage**\n   <!-- - The big data interchange problem\n   - Row vs. column storage formats\n   - Introduction to Parquet and Arrow -->\n\n2. **Deep Dive into Parquet** \n   <!-- - Key features and benefits\n   - Performance benchmarks\n   - Demo: Efficient dataset storage -->\n\n3. **Working with Arrow in R** \n   <!-- - The Arrow package\n   - Reading/writing Parquet files\n   - Using dplyr with Arrow tables -->\n\n4. **Querying Parquet with Different Engines** \n   <!-- - DuckDB: SQL queries on Parquet files\n   - Data.table approach\n   - Arrow query execution\n   - Performance comparisons -->\n\n5. **Arrow Datasets for Larger-than-Memory Operations** \n   <!-- - Datasets vs. Tables\n   - Handling data too large for memory\n   - Working with datasets on S3 -->\n\n6. **Partitioning Strategies** \n   <!-- - Understanding partitioning\n   - Hive vs. non-Hive partitioning\n   - Best practices -->\n\n7. **Hands-on Workshop: Analysis with PUMS Data** \n   <!-- - Reading partitioned Parquet files\n   - Executing queries with DuckDB and Arrow\n   - Practice exercises -->\n\n# Introduction to Column-Oriented Data Storage\n\n## Why should I care about data storage?\n\n::: {.fragment .extrapad}\nData has to be represented somewhere, both during analysis and when storing.\n:::\n\n::: {.fragment .extrapad}\nThe shape and characteristics of this representation has a huge impact on performance.\n:::\n\n::: {.fragment .extrapad}\nWhat if you could speed up a key part of your analysis by 30x and reduce your storage by 10x?\n:::\n\n## Row vs. Column-Oriented Storage\n\n::: columns\n::: {.column width=\"50%\"}\n**Row-oriented**\n\n```\n|ID|Name |Age|City    |\n|--|-----|---|--------|\n|1 |Alice|25 |New York|\n|2 |Bob  |30 |Boston  |\n|3 |Carol|45 |Chicago |\n```\n\n- Efficient for single record access\n- Efficient for appending\n:::\n\n::: {.column width=\"50%\"}\n::: {.fragment}\n**Column-oriented**\n\n```\nID:    [1, 2, 3]\nName:  [Alice, Bob, Carol]\nAge:   [25, 30, 45]\nCity:  [New York, Boston, Chicago]\n```\n\n- Efficient for analytics\n- Better compression\n:::\n:::\n:::\n\n::: {.notes}\nRow oriented formats are super familiar: CSVs as well as many databases\n\nBut Column-orientation isn't something that is new and cutting edge. In fact, every single one of you use a system that stores data this way: R data frames(!)\n:::\n\n## Why Column-Oriented Storage?\n\n::: {.incremental}\n- **Analytics typically access a subset of columns**\n  - \"What is the average age by city?\"\n  - Only needs [Age, City] columns\n\n- **Benefits:**\n  - Only read needed columns from disk\n  - Similar data types stored together\n  - Better compression ratios\n:::\n\n::: {.notes}\nCompression: this is because like-types are stored with like, so you get more frequent patterns — the core of compression. But you also can use encodings like dictionary encodings very efficiently.\n:::\n\n## Column-Oriented Data is great \n\n:::{.extrapad}\nAnd you use column-oriented dataframes already!\n:::\n\n::: {.fragment}\n... but still storing my data in a fundamentally row-oriented way. \n:::\n\n::: {.notes}\nThis isn't so bad if you're only talking about a small amount of data, transposing a few columns for a few rows is no big deal. But as data gets larger, or if you have to do this frequently, this process of transposing (AKA serialization) hurts.\n:::\n\n## The interconnection problem\n\n![](images/copy-convert.png){.r-stretch}\n\n::: {.notes}\nMany of these were operating in essentially column-oriented ways — but to transfer data ended up writting into row-oriented data structures, then read them back in to something that was column-oriented.\n\n**Moving data between representations is hard**\n  - Different formats, requirements, and limitations\n  - Compatibility issues\n  - Serialization is a huge bottleneck\n:::\n\n## The interconnection problem\n\n![](images/shared.png){.r-stretch}\n\n## What is Apache Arrow?\n\n![](images/arrow-logo_vertical_black-txt_transparent-bg.png){width=50%}\n\n::: {.incremental}\n- **Cross-language development platform for in-memory data**\n  - Consistent in-memory columnar data format\n  - Language-independent\n  - Zero-copy reads\n\n- **Benefits:**\n  - Seamless data interchange between systems\n  - Fast analytical processing\n  - Efficient memory usage\n:::\n\n## What is Apache Parquet?\n\n![](images/Apache_Parquet_logo.svg){width=40%}\n\n::: {.incremental}\n- **Open-source columnar storage format**\n  - Created by Twitter and Cloudera in 2013\n  - Part of the Apache Software Foundation\n\n- **Features:**\n  - Columnar storage\n  - Efficient compression\n  - Explicit schema \n  - Statistical metadata\n:::\n\n## Reading a File\n\nAs a CSV file\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time({\n  df <- read.csv(\"CA_person_2021.csv\")\n})\n```\n:::\n\n:::{.fragment .extrapad}\n```\n   user  system elapsed \n 14.449   0.445  15.037 \n```\n:::\n\n:::{.notes}\nDescribe the CSV\n\nThis CSV is 708 MB, I'm reading this in on my MacBook Pro, your times will vary! We can use arrow or data.tables's CSV reader and it's faster (1.85 sec and 1.61 sec respectively). And if we read to an arrow table it's even faster: 0.51 seconds\n:::\n\n## Reading a File\n\nAs a Parquet file\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2|\"}\nlibrary(arrow)\noptions(arrow.use_altrep = FALSE)\n\nsystem.time({\n  df <- read_parquet(\"CA_person_2021.parquet\")\n})\n```\n:::\n\n:::{.fragment .extrapad}\n```\n   user  system elapsed \n  1.017   0.207   0.568 \n```\n:::\n\n:::{.notes}\nThe parquet file is 62 MB\n\nIt's even faster with altrep (0.186 s), but that's cheating! Also, if we read into an arrow table rather than a dataframe: 0.1 second \n:::\n\n# Deep Dive into Parquet\n\n## What is Parquet?\n\n::: {.incremental}\n- **Schema metadata**\n  - Self-describing format\n  - Preserves column types\n  - Type-safe data interchange\n\n- **Encodings**\n  - **Dictionary** — Particularly effective for categorical data\n  - **Run-length encoding** - Efficient storage of sequential repeated values\n\n- **Advanced compression**\n  - Column-specific compression algorithms\n  - Both dictionary and value compression\n:::\n\n## Exercise\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- tibble::tibble(\n  integers = 1:10,\n  doubles = as.numeric(1:10),\n  strings = sprintf(\"%02d\", 1:10)\n)\n\nwrite.csv(data, \"numeric_base.csv\", row.names = FALSE)\nwrite_csv_arrow(data, \"numeric_arrow.csv\")\nwrite_parquet(data, \"numeric.parquet\")\n\ndf_csv <- read.csv(\"numeric_base.csv\")\ndf_csv_arrow <- read_csv_arrow(\"numeric_arrow.csv\")\ndf_parquet <- read_parquet(\"numeric.parquet\")\n```\n:::\n\n\n::: {.fragment .extrapad}\nAre there any differences?\n:::\n\n## Exercise\n\n:::{.columns}\n:::{.column}\n```{.default}\n> df_csv_arrow\n# A tibble: 10 × 3\n   integers doubles strings\n      <int>   <int>   <int>\n 1        1       1       1\n 2        2       2       2\n 3        3       3       3\n 4        4       4       4\n 5        5       5       5\n 6        6       6       6\n 7        7       7       7\n 8        8       8       8\n 9        9       9       9\n10       10      10      10\n```\n:::\n\n:::{.column}\n```{.default}\n> df_parquet\n# A tibble: 10 × 3\n   integers doubles strings\n      <int>   <dbl> <chr>  \n 1        1       1 01     \n 2        2       2 02     \n 3        3       3 03     \n 4        4       4 04     \n 5        5       5 05     \n 6        6       6 06     \n 7        7       7 07     \n 8        8       8 08     \n 9        9       9 09     \n10       10      10 10     \n```\n:::\n:::\n\n## Exercise\n\n:::{.columns}\n:::{.column}\n```{.default code-line-numbers=\"3-4\"}\n> df_csv_arrow\n# A tibble: 10 × 3\n   integers doubles strings\n      <int>   <int>   <int>\n 1        1       1       1\n 2        2       2       2\n 3        3       3       3\n 4        4       4       4\n 5        5       5       5\n 6        6       6       6\n 7        7       7       7\n 8        8       8       8\n 9        9       9       9\n10       10      10      10\n```\n:::\n\n:::{.column}\n```{.default code-line-numbers=\"3-4\"}\n> df_parquet\n# A tibble: 10 × 3\n   integers doubles strings\n      <int>   <dbl> <chr>  \n 1        1       1 01     \n 2        2       2 02     \n 3        3       3 03     \n 4        4       4 04     \n 5        5       5 05     \n 6        6       6 06     \n 7        7       7 07     \n 8        8       8 08     \n 9        9       9 09     \n10       10      10 10     \n```\n:::\n:::\n\n## Inside a Parquet File\n\n![](images/files_formats_parquet_bw.png){width=100%}\n\n::: {.notes}\n- **Row groups**: Horizontal partitions of data\n- **Column chunks**: Columnar data within a row group\n- **Pages**: Small units of column chunk data\n- **Footer**: Contains file metadata and schema\n:::\n\n## Benchmarks: Parquet vs CSV\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-arrow_files/figure-revealjs/unnamed-chunk-4-1.png){width=408}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-arrow_files/figure-revealjs/unnamed-chunk-5-1.png){width=408}\n:::\n:::\n\n:::\n:::\n\n## Benchmarks: Parquet vs CSV\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-arrow_files/figure-revealjs/unnamed-chunk-6-1.png){width=720}\n:::\n:::\n\n\n## Reading Efficiency: Selecting Columns\n\n::: {.incremental}\n- **With CSV:**\n  - Must read entire file, even if you only need a few columns\n  - No efficient way to skip columns during read\n\n- **With Parquet:**\n  - Read only needed columns from disk\n  - Significant performance benefit for wide tables\n\n::: {.fragment}\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time({\n  df_subset <- read_parquet(\n    \"CA_person_2021.parquet\", \n    col_select = c(\"PUMA\", \"COW\")\n  )\n})\n```\n:::\n\n:::\n\n::: {.columns}\n::: {.column .fragment}\n```\n   user  system elapsed \n  0.027   0.003   0.031 \n```\n:::\n::: {.column .fragment}\n```\n   user  system elapsed \n  1.017   0.207   0.568 \n```\n:::\n:::\n:::\n\n\n## nanoparquet vs. arrow Reader\n\n::: {.incremental}\n- **nanoparquet**\n  - Lightweight Parquet reader\n  - Minimal dependencies\n  - Good for embedding\n\n- **arrow**\n  - Full-featured reader\n  - Support for datasets\n  - Integration with Arrow ecosystem\n  - Optimized for analytics workloads\n:::\n\n## nanoparquet vs. arrow Reader\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\noptions(arrow.use_altrep = FALSE)\n\nsystem.time({\n  df <- read_parquet(\"CA_person_2021.parquet\")\n})\n```\n:::\n\n\n```\n   user  system elapsed \n  1.017   0.207   0.568 \n```\n\n:::{.extrapad}\n<br/>\n:::\n\n::: {.fragment}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nanoparquet)\n\nsystem.time({\n  df <- read_parquet(\"CA_person_2021.parquet\")\n})\n```\n:::\n\n\n```\n   user  system elapsed \n  0.709   0.099   0.894 \n```\n:::\n\n\n\n## Parquet Tooling Ecosystem\n\n**Languages with native Parquet support:**\n\n::: {.incremental}\n- R (via arrow)\n- Python (via pyarrow, pandas)\n- Java\n- C++\n- Rust\n- JavaScript\n- Go\n:::\n\n## Parquet Tooling Ecosystem\n\n**Systems with Parquet integration:**\n\n::: {.incremental}\n- Apache Spark\n- Apache Hadoop\n- Apache Drill\n- Snowflake\n- Amazon Athena\n- Google BigQuery\n- DuckDB\n:::\n\n# Working with Parquet files with Arrow in R\n\n## Introduction to the arrow Package\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load the Arrow package\ninstall.packages(\"arrow\")\nlibrary(arrow)\n\n# Check Arrow version and capabilities\narrow_info()\n```\n:::\n\n\n::: {.incremental}\n- The **arrow** package provides:\n  - Native R interface to Apache Arrow\n  - Tools for working with large datasets\n  - Integration with dplyr for data manipulation\n  - Reading/writing various file formats\n:::\n\n## Reading and Writing Parquet files, revisited\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1-2|4-5|7-11|13-16\"}\n# Read a Parquet file into R\ndata <- read_parquet(\"CA_person_2021.parquet\")\n\n# Write an R data frame to Parquet\nwrite_parquet(data, \"CA_person_2021_new.parquet\")\n\n# Reading a subset of columns\ndf_subset <- read_parquet(\n  \"CA_person_2021.parquet\", \n  col_select = c(\"PUMA\", \"COW\", \"AGEP\")\n)\n\n# Reading with a row filter (predicate pushdown)\ndf_filtered <- open_dataset(\"CA_person_2021.parquet\") |> \n  filter(AGEP > 40) |>\n  collect()\n```\n:::\n\n\n## Demo: Using dplyr with arrow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create an Arrow Table\ndf <- read_parquet(\"CA_person_2021.parquet\", as_data_frame = FALSE)\n\n# Use dplyr verbs with arrow tables\ndf |>\n  filter(AGEP >= 16) |>\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) \n```\n:::\n\n\n::: {.notes}\nThe dataframe is backed by altrep, actually. But generally functions like any other dataframe.\n:::\n\n# Querying Parquet with Different Engines\n\n## Introduction to DuckDB\n\n![](https://duckdb.org/images/duckdb-circle.svg){width=25%}\n\n::: {.incremental}\n- **Analytical SQL database system**\n  - Embedded database (like SQLite)\n  - Column oriented\n  - In-process query execution\n\n- **Key features:**\n  - Direct Parquet querying\n  - Vectorized query execution\n  - Parallel processing\n  - Zero-copy integration with arrow\n:::\n\n:::{.notes}\nThe zero-copy integration with arrow is because DuckDB uses basically the same format for it's own internal representation.\n:::\n\n## DuckDB\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckdb)\n\ncon <- dbConnect(duckdb())\n\n# Register a Parquet file as a virtual table\ndbExecute(con, \"CREATE VIEW pums AS SELECT * \n                FROM read_parquet('CA_person_2021.parquet')\")\n\n# Run our query\ndbGetQuery(con, \"\n  SELECT SUM(JWMNP * PWGTP)/SUM(PWGTP) as avg_commute_time,\n         COUNT(*) as count\n  FROM pums\n  WHERE AGEP >= 16\n\")\n\ndbDisconnect(con, shutdown = TRUE)\n```\n:::\n\n\n## duckplyr\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckplyr)\n\n# Read data with Arrow\npums_data <- read_file_duckdb(\n  \"CA_person_2021.parquet\", \n  \"read_parquet\"\n)\n\n# Use duckplyr to optimize dplyr operations\npums_data |>\n  filter(AGEP >= 16) |>\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |>\n  collect()\n```\n:::\n\n\n:::{.notes}\nduckplyr is a drop-in replacement for dplyr, using duckdb as a backend\n:::\n\n## data.table\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nlibrary(data.table)\n\n# Read Parquet file with Arrow\npums_data <- read_parquet(\"CA_person_2021.parquet\")\n\n# Convert to data.table\npums_dt <- as.data.table(pums_data)\n\n# data.table query\npums_dt[AGEP >= 16,\n  .(avg_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP), \n    count = .N)]\n```\n:::\n\n\n## Arrow Query Execution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create an Arrow Dataset\npums_ds <- open_dataset(\"pums_dataset_dir/\")\n\n# Execute query with Arrow\nresult <- pums_ds |>\n  filter(AGEP >= 16) |>\n  group_by(ST) |>\n  summarize(\n    avg_commute_time = mean(JWMNP, na.rm = TRUE),\n    count = n()\n  ) |>\n  arrange(desc(avg_commute_time)) |>\n  collect()\n```\n:::\n\n\n## Demo: Seamless Integration Arrow ↔ DuckDB\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read_parquet(\"CA_person_2021.parquet\")\n\n# Use dplyr verbs with arrow tables\ndf |>\n  filter(AGEP >= 16) |>\n  to_duckdb() |>\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) \n```\n:::\n\n\n# Arrow Datasets for Larger-than-Memory Operations\n\n## Understanding Arrow Datasets vs. Tables\n\n::: columns\n::: {.column width=\"50%\"}\n**Arrow Table**\n\n- In-memory data structure\n- Must fit in RAM\n- Fast operations\n- Similar to base data frames\n- Good for single file data\n:::\n\n::: {.column width=\"50%\"}\n**Arrow Dataset**\n\n- Collection of files\n- Lazily evaluated\n- Larger-than-memory capable\n- Distributed execution\n- Supports partitioning\n:::\n:::\n\n## Demo: Opening and Querying Multi-file Datasets\n\n\n::: {.cell}\n\n```{.r .cell-code}\npums_ds <- open_dataset(\"data/person\")\n\n# Examine the dataset, list files\nprint(pums_ds)\nhead(pums_ds$files)\n\n# Query execution with lazy evaluation\npums_ds |>\n  filter(AGEP >= 16) |>\n  group_by(year, ST) |>\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |>\n  collect()\n```\n:::\n\n\n## Lazy Evaluation and Query Optimization\n\n::: {.incremental}\n- **Lazy evaluation workflow:**\n  1. Define operations (filter, group, summarize)\n  2. Arrow builds an execution plan\n  3. Optimizes the plan (predicate pushdown, etc.)\n  4. Only reads necessary data from disk\n  5. Executes when `collect()` is called\n\n- **Benefits:**\n  - Minimizes memory usage\n  - Reduces I/O operations\n  - Leverages Arrow's native compute functions\n:::\n\n## Working with Datasets on S3\n\narrow can work with data and datasets in cloud storage. This can be a good option if you don't have access to a formal DBMS.\n\n::: {.incremental}\n- Easy to store\n- arrow efficiently uses metadata to read only what is necessary \n:::\n\n:::{.notes}\nI know, I know — this workshop is about **local** files. But I couldn't help myself\n:::\n\n## Demo: Working with Datasets on S3\n\n\n::: {.cell}\n\n```{.r .cell-code}\npums_ds <- open_dataset(\"s3://scaling-arrow-pums/person/\")\n\n# Query execution with lazy evaluation\npums_ds |>\n  filter(year == 2021, location == \"ca\", AGEP >= 16) |>\n  group_by(year, ST) |>\n  summarize(\n    mean_commute_time = sum(JWMNP * PWGTP, na.rm = TRUE) /\n      sum(PWGTP),\n    count = n()\n  ) |>\n  collect()\n```\n:::\n\n:::{.notes}\nTalk about partitioning helping, etc \n:::\n\n## Demo: Sipping data\n\n\n::: {.cell}\n\n```{.r .cell-code}\npums_ds <- open_dataset(\"s3://scaling-arrow-pums/person/\")\n\n# Query execution with lazy evaluation\npums_ds |>\n  filter(AGEP >= 97) |>\n  collect()\n```\n:::\n\n\n:::{.notes}\n`Sys.getpid()`\n`nettop -p X`\n:::\n\n# Partitioning Strategies\n\n## What is Partitioning?\n\n::: {.incremental}\n- **Dividing data into logical segments**\n  - Stored in separate files/directories\n  - Based on one or more column values\n  - Enables efficient filtering\n\n- **Benefits:**\n  - Faster queries that filter on partition columns\n  - Improved parallel processing\n  - Easier management of large datasets\n:::\n\n![](https://arrow.apache.org/docs/r/_images/dataset-parquet-partition.svg){width=50%}\n\n## Hive vs. Non-Hive Partitioning\n\n::: columns\n::: {.column width=\"50%\"}\n**Hive Partitioning**\n\n- Directory format: `column=value`\n- Example:\n  ```\n  person/\n  ├── year=2018/\n  │   ├── state=NY/\n  │   │   └── data.parquet\n  │   └── state=CA/\n  │       └── data.parquet\n  ├── year=2019/\n  │   ├── ...\n  ```\n- Self-describing structure\n- Standard in big data ecosystem\n:::\n\n::: {.column width=\"50%\"}\n**Non-Hive Partitioning**\n\n- Directory format: `value`\n- Example:\n  ```\n  person/\n  ├── 2018/\n  │   ├── NY/\n  │   │   └── data.parquet\n  │   └── CA/\n  │       └── data.parquet\n  ├── 2019/\n  │   ├── ...\n  ```\n- Requires column naming\n- Less verbose directory names\n:::\n:::\n\n## Effective Partitioning Strategies\n\n::: {.incremental}\n- **Choose partition columns wisely:**\n  - Low to medium cardinality\n  - Commonly used in filters\n  - Balanced data distribution\n\n- **Common partition dimensions:**\n  - Time (year, month, day)\n  - Geography (country, state, region)\n  - Category (product type, department)\n  - Source (system, sensor)\n:::\n\n## Partitioning in Practice: Writing Datasets\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|13-16\"}\nca_pums_data <- read_parquet(\"CA_person_2021.parquet\")\n\nca_pums_data |>\n  mutate(\n    age_group = case_when(\n      AGEP < 18 ~ \"under_18\",\n      AGEP < 30 ~ \"18_29\",\n      AGEP < 45 ~ \"30_44\",\n      AGEP < 65 ~ \"45_64\",\n      TRUE ~ \"65_plus\"\n    )\n  ) |>\n  group_by(ST, age_group) |>\n  write_dataset(\n    path = \"ca_pums_by_age/\"\n  )\n```\n:::\n\n\n## Demo: repartitioning the whole dataset\n\n::: {.cell}\n\n```{.r .cell-code}\npums_data <- open_dataset(\"data/person\")\n\npums_data |>\n  mutate(\n    age_group = case_when(\n      AGEP < 18 ~ \"under_18\",\n      AGEP < 30 ~ \"18_29\",\n      AGEP < 45 ~ \"30_44\",\n      AGEP < 65 ~ \"45_64\",\n      TRUE ~ \"65_plus\"\n    )\n  ) |>\n  group_by(year, ST, age_group) |>\n  write_dataset(\n    path = \"pums_by_age/\"\n  )\n```\n:::\n\n\n## Best Practices for Partition Design\n\n::: {.incremental}\n- **Avoid over-partitioning:**\n  - Too many small files = poor performance\n  - Target file size: 20MB–2GB\n  - Avoid high-cardinality columns (e.g., user_id)\n\n- **Consider query patterns:**\n  - Partition by commonly filtered columns\n  - Balance between read speed and write complexity\n\n- **Nested partitioning considerations:**\n  - Order from highest to lowest selectivity\n  - Limit partition depth (2-3 levels typically sufficient)\n:::\n\n## Partitioning Performance Impact\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-arrow_files/figure-revealjs/unnamed-chunk-23-1.png){width=768}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(\"<path/to/data>\") |>\n  filter(year >= 2018) |>\n  summarise(\n    mean_commute = sum(JWMNP * PWGTP, na.rm = TRUE) / sum(PWGTP)\n  ) |>\n  collect()\n```\n:::\n\n\n# [Hands-on practice: Analysis with PUMS Data](../activities/03-practice-arrow.qmd)\n\n## Conclusion\n\n:::{.incremental}\n- Column-oriented storage formats like Parquet provide massive \nperformance advantages for analytical workloads (30x speed, 10x smaller\nfiles)\n- Apache Arrow enables seamless data interchange between systems without\ncostly serialization/deserialization\n- Multiple query engines (arrow, DuckDB, data.table) offer flexibility\ndepending on your analysis needs, all using modern formats like Parquet\n- Partitioning strategies help manage large datasets effectively when\nworking with data too big for memory\n:::\n\n## Conclusion\n\n**Resources:**\n\n- Workshop materials: [GitHub Repository](https://github.com/your-repo)\n- Arrow documentation: [arrow.apache.org/docs/r](https://arrow.apache.org/docs/r/)\n- Parquet: [parquet.apache.org](https://parquet.apache.org/)\n- DuckDB: [duckdb.org](https://duckdb.org/)\n- Book: [Scaling up with Arrow and R](https://arrow-user2022.github.io/scaling-r-with-arrow/)\n\n**Questions?**\n",
    "supporting": [
      "03-arrow_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}